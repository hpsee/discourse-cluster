{
    "210": [
        "<p>This is a category for topics which are of interest to the research computing community, but not likely to have a \u201cbest\u201d answer.  We will likely create subcategories here over time.</p>"
    ],
    "1040": [
        "<p>How can we obtain the metrics of performance and computational requirements for a<br>\ncompute-heavy workload that must run on Windows?</p>\n<p>Background: This question is related to how we can size a VM \u201cjust right\u201d for a given computational workload. And we want to make it easy for researchers to obtain these metrics on their own.</p>\n<p>Some scoping would be helpful: I am interested specifically in compute-heavy workloads that will be run under a VM, but it does not have to be so. Why VM? Because sometimes that\u2019s what available to us\u2013no access to bare metals. Let\u2019s limit ourselves to Windows 8 and up.</p>\n<p>I understand that Windows have a built-in Task Manager and it can show CPU utilization (per core) and memory usage (total and per-application). I tend to doubt whether the per-application memory usage shown on the Task Manager is really honest. Any experience on this? Is there a tool to obtain CPU &amp; memory utilization of an application as a function of time? Do we only have the option of watching the \u201ctask manager movie\u201d as the computation goes on?</p>\n<p>Wirawan</p>"
    ],
    "797": [
        "<p>We have some projects that include running calculations with high I/O on our university HPC cluster.  We are considering moving them to cloud, probably AWS.  I believe that the data to be used will reside in a data center in the cloud, suggesting that latency and bandwidth could possibly be affected by geological distances.  Not to mention that properties and capacities on both the client side and the cloud storage end will most likely affect performance as observed by the user running the job.</p>\n<p>What are the effects of the level of parallelization of the storage itself?  Most likely this depends on the size of the chunks of data (objects) being requested (as well as the frequency). The client configuration; processing speed, memory, its own storage properties; must also influence I/O performance in the cloud.  Does anyone have some recent numbers (and impressions) they could share?</p>\n<p>Thank you!</p>"
    ],
    "1081": [
        "<p>I know of individual research support staff that have experience optimizing code, or have proactively reached out to specific users to do so, and it seems like something that should be so easy to provide as a service - e.g., - the user can add a flag to some job, the job will be run as normal but with some recording of overall usage metrics, it gets sent to support staff to analyze, and then a report (and possible suggestions are returned, given that the user is able to share code after some back and forth).</p>\n<p>Is anyone doing this? If not, why not, and how could we automate it? Assume we have a large group of users, and some cluster resource with support staff and a job manager. What tools could be used, and how might it work?</p>\n<p>What brought this to mind is this post: <a href=\"https://easyperf.net/blog/2019/10/05/Performance-Analysis-Of-MT-apps\" rel=\"nofollow noopener\">https://easyperf.net/blog/2019/10/05/Performance-Analysis-Of-MT-apps</a></p>",
        "<p>Just brainstorming here, but maybe sys admins could create a post execution script that returns some data on usage on all nodes, such as the output of \u201csar\u201d in Linux.  I\u2019ve used that before to determine what is happening with CPU, RAM, etc. during different phases of a code running.  It is a bit tedious to try to stitch all the information together.  Your post has me thinking as I\u2019ve wanted to profile some of what gets run to determine if we can optimize how the nodes are running.</p>",
        "<p>That\u2019s a cool idea! Here is sar for those interested:</p>\n<p><a href=\"https://linux.die.net/man/1/sar\" class=\"onebox\" target=\"_blank\" rel=\"nofollow noopener\">https://linux.die.net/man/1/sar</a></p>\n<p>I\u2019m frustrated about this topic, because unlike a lot of things, I\u2019m not actually empowered to do or try anything. It comes down to a game of trying to convince those that are, and I always lose. Does anyone have a cluster resource that is open to experimenting / trying new things that I can help with?</p>",
        "<p>I had some quick discussion with folks from my team, and while we don\u2019t have bandwidth to develop this, I wanted to share the knowledge in the case / in hopes that if someone did have interest and time, they could have the idea!</p>\n<p>Technically, if nodes are between jobs, this makes profiling a bit more complicated but not impossible. We also must take into account having different filesystems. For the major filesystems, it should be done at the Lustre server side, using job statistics provided by Lustre</p>\n<p><a href=\"https://build.whamcloud.com/job/lustre-manual/lastSuccessfulBuild/artifact/lustre_manual.xhtml#idm139901745537120\" class=\"onebox\" target=\"_blank\" rel=\"nofollow noopener\">https://build.whamcloud.com/job/lustre-manual/lastSuccessfulBuild/artifact/lustre_manual.xhtml#idm139901745537120</a></p>\n<p>and bound to Slurm Job IDs.</p>\n<ul>\n<li>On the  <strong>backend</strong> , we would need (1) a scalable way to gather these metrics from all Lustre servers (a few tens of servers max, so not big deal), then (2) feed a database with the data;</li>\n<li>On the  <strong>frontend</strong> , we need a SSO-enabled web interface to display the data per group/user, likely controlled by Stanford Workgroup (or in your case, whatever identity management / groups your institution uses).</li>\n</ul>\n<p>A user could then see their job execution time and all I/O stats performed during the job. That\u2019s the general idea. We likely need a small (virtualized) Slurm+Lustre test platform to develop this.</p>",
        "<p>There are a variety of profiling and optimization tools that could be brought to bear on this kind of problems and I\u2019ve used a lot of them, but what I see mostly is not application optimization bottlenecks such as in the link you provided, but application bottlenecks due to simple IO, often caused by specifying the wrong device for temp files or not pre-staging data to an appropriate device (running tiny, but heavy IO back to a main cluster filesystem instead of using /dev/shm or a local NVME FS).  The other problem I see quite often is interfering resource usage where one user\u2019s app is hosing a multi-CPU server, or indeed the entire cluster\u2019s FS via unthinking launching of their app with default options (which is not surprising since there\u2019s no support for teaching any of this to incoming grad students on an official basis).<br>\nI\u2019ve come up with <a href=\"http://moo.nac.uci.edu/~hjm/hpc/profilemyjobs/profilemyjobs.html\" rel=\"nofollow noopener\">profilemyjobs</a>, an unholy goulash of bash and gnuplot that allows users to record long-running processes and also visualize it simultaneously.  It records about 30 performance parameters (not all visualized at the same time) and plots them on the same screen so you can see which ones interact without switching screens back and forth, which is a failing of some of the more sophisticated tools available like PCP, XDMoD, etc.  It also logs those variables, and autogenerates the gnuplot file to review the plot afterwards.  There\u2019s a separate gnuplot pane for detailed disk IO if wanted.  Also a wrapper called pmj to submit the whole thing in a batch job (obviously not for same-time viz).</p>\n<p>It was an experiment to see how easy it would be to use bash instead of a real programming language to do something useful.  I won\u2019t do that again.  But it is straight bash, so it ought to be quite portable.  Not meant for nanosecond profiling, but for minutes/hours/days profiling.</p>\n<p>I came to depend on it for cluster admin as well as debugging overloaded apps, slow IO, memory overflows/constrictions, bad SGE params, etc.</p>\n<p>harry</p>"
    ],
    "1079": [
        "<p>When you\u2019re testing a script, or running jobs on a cluster, things can go wrong. You get an ugly error message, your script exists, and then what? Part of learning to be a researcher or student is knowing how to ask for help. With this in mind, let\u2019s talk about all the different ways that you might ask:</p>\n<ul>\n<li>attending office hours to interact with support staff at your institution</li>\n<li>emailing support staff directly</li>\n<li>submitting a ticket to a help desk</li>\n<li>using a command line tool to submit a help request (e.g., <a href=\"https://vsoch.github.io/helpme/helper-discourse\" rel=\"nofollow noopener\">helpme</a>)</li>\n<li>opening up an issue on a bug tracker (e.g., GitHub)</li>\n</ul>\n<p>If you are a student - what are your favorite ways to ask for help? How quickly do you get a response, what is the quality of the response, and how do those two things relate? How could support be better?</p>\n<p>On the flip side, if you are a provider of help (a maintainer on a GitHub board or a support staff) what kind of tools or services make your life easier? Is there a format or a tool that works really well, or that doesn\u2019t?</p>\n<p>For this question - for support providers and receivers alike - let\u2019s talk about how we can do this better. If you could design the perfect interaction, what would it look like?</p>",
        "<p>As a support provider, the two main issues I have with users is that they never provide enough information related to their problem, and never read our documentation.</p>\n<p>Issue <span class=\"hashtag\">#1</span>, they email us directly with something like \u201cHey, my job didn\u2019t run, can you take a look?\u201d I email back with the common questions, what is your username, what is the location of your work directory, where are your job scripts, etc\u2026 3 to 4 emails later I finally have enough information to diagnose the problem. Central IT has a ticket system but users rarely (if ever) use it and they just email us directly.</p>\n<p>Issue <span class=\"hashtag\">#2</span>, most of the problems that arise from their jobs we have documented in our support wiki. I am constantly sending the link to users (new and old) and politely suggesting them to read the wiki.</p>",
        "<p><a class=\"mention\" href=\"/u/jeremymann\">@jeremymann</a> you might be interested in a tool like HelpMe then - it will submit the user environment and a screen recording to a service of your choosing (e.g., UserVoice is our ticketing system <a href=\"https://vsoch.github.io/helpme/helper-uservoice\" rel=\"nofollow noopener\">https://vsoch.github.io/helpme/helper-uservoice</a>) and there is also support for GitHub and Discourse. If you have some other system with an API I\u2019d be happy to add that as an integration! And we can also customize the helper further to have some other kind of recorder (aside from terminal asciinema recording) or collector (aside from the environment).</p>",
        "<p>I\u2019m a support provider as well.  We have a help desk system, and we usually insist that people put in a ticket or at least put in one afterwards if it is an emergency.  I\u2019ve found that our support organization has a similar issue to what Jeremy mentioned. We might get a ticket with very vague information, and then we have to try and figure out. Or we get led down a path because someone thinks it is one thing but actually another. For instance, someone might say the code said the network was unavailable, but their job actually used up RAM and the kernel killed it.</p>\n<p>We also find that users of our clusters share information among themselves on how to set up their account to run a code. Most times it is correct, but when it isn\u2019t, bad practice starts to spread.</p>\n<p>I worked in engineering before moving to HPC application / Linux support.  Because of that I find it easy to talk to our users. Simply going and making \u201chouse calls\u201d and following up on \u201chow are things working?\u201d has helped to give a good impression of our group. I think in HPC developing relationships with the users has helped me immensely in providing support.</p>\n<p>My favorite question is: \u201cWhat are you trying to do?\u201d or \u201cWhat problem are you trying to solve?\u201d.  It allows me to get into the deeper aspects of how they are trying to run codes on the system.</p>",
        "<p>For both of you, what\u2019s the ratio between users and support? I like the idea of house calls, but I\u2019d imagine that gets harder to do when you have fewer support staff per users on campus.</p>",
        "<p>I would say it is a pretty low number of support to users.  We do our best.  Most codes our users run are off the shelf so most support is when either someone is getting an account setup, trying to run a new code, or something breaks.</p>",
        "<p>We have two methods for our people to ask for support, they may email our support email address (which we advertise all over), or they may put in a service ticket directly to our team.</p>",
        "<p>I sympathize with any support crew who play email ping-pong to try to extract the relevant information from a user who\u2019s requesting help.  In order to speed up the process, I\u2019ve written 2 things; one is a HOWTO: <a href=\"http://moo.nac.uci.edu/~hjm/HOWTO_Ask_a_question.html\" rel=\"nofollow noopener\">How to ask a question</a> (which I\u2019ve added to an email reply template which I use to smack the user if the initial request isn\u2019t enough to start work on the problem) and the other is  a simple bash script called <a href=\"http://moo.nac.uci.edu/~hjm/hpc/HPC-Mayday.html\" rel=\"nofollow noopener\">mayday</a>\u2026</p>\n<p>The second is referenced in the first, but it\u2019s worth noting separately.  Being pure bash, it should not be hard to port to other clusters.  It also uses the very handy termbin to keep emails reasonably short.  YMMV whether it works or you, but I\u2019d be interested in providing mods to other users if they have other systems that need to be incorporated (SLURM instead of SGE, etc)</p>\n<p>best<br>\nharry</p>"
    ],
    "1073": [
        "<p>Good afternoon everyone,</p>\n<p>I have a faculty member that is interested in introducing their anatomy and physiology students to supercomputing. They are looking for ideas that could be useful to explore in the classroom or have students start working on as actual research in the lab. I have done a surface Google investigation and found a few ideas, but we would really value the input from our community here.</p>\n<p>Thank you for taking the time to read this!</p>\n<p>Adam Erck</p>",
        "<p>A BLAST example or exercise is always a good introduction to bioinformatics for students, this is a good exercise using CLI BLAST: <a href=\"http://hpc.ilri.cgiar.org/beca/training/AdvancedBFX2014_2/course/CommandLineBLAST.pdf\" rel=\"nofollow noopener\">http://hpc.ilri.cgiar.org/beca/training/AdvancedBFX2014_2/course/CommandLineBLAST.pdf</a></p>\n<p>Another good application for A&amp;P would be a proteomics exercise such as this, but this might be a bit over the heads of sophomore undergrads unless they\u2019ve already studied mass spec: <a href=\"https://community.asdlib.org/activelearningmaterials/biological-mass-spectrometry-proteomics/\" rel=\"nofollow noopener\">https://community.asdlib.org/activelearningmaterials/biological-mass-spectrometry-proteomics/</a></p>",
        "<p>Thank you for the ideas!</p>",
        "<p>Gateways may be a quick way to get some simulations going.</p>\n<p>Simvascular Gateway provides cardio vascular modeling. Please let them check out <a href=\"https://gateway.simvascular.org/\" rel=\"nofollow noopener\">https://gateway.simvascular.org/</a>.</p>\n<p>There are others like <a href=\"https://www.nsgportal.org/index.html\" rel=\"nofollow noopener\">https://www.nsgportal.org/index.html</a> for neuroscience. I know there are faculty modeling bone  structure and mechanics using HPC like Prof Iwona Jasuik at UIUC.</p>\n<p>Thanks,</p>\n<p>Sudhakar.</p>",
        "<p>Hi, Adam,</p>\n<p>The Virtual Soldier Research Program at the University of Iowa \u201cconducts basic and applied research and development in the field of human modeling and simulation. Our research is aimed at creating interactive, intelligent, and predictive human models that operate in virtual, physics-based environments. The product from this research is called Santos, a human simulator that is widely used by the US military and industry partners. It is the only physics-based human simulator\u201d: <a href=\"https://www.ccad.uiowa.edu/vsr/\" rel=\"nofollow noopener\">https://www.ccad.uiowa.edu/vsr/</a></p>\n<p>The Iowa initiative leverages pioneering work by PI Brian Athey (U-Michigan) made possible with a DARPA investment, and several follow-on projects that were funded by NIH, National Library of Medicine, and others. <a href=\"https://medicine.umich.edu/dept/dcmb/brian-d-athey-phd\" rel=\"nofollow noopener\">https://medicine.umich.edu/dept/dcmb/brian-d-athey-phd</a></p>\n<p>E</p>",
        "<p>Hello Adam,</p>\n<p>Perhaps obvious choices for many people would be HPC applications involving the NIH-supported Human Connectome Project (<a href=\"https://neuroscienceblueprint.nih.gov/human-connectome/connectome-programs\" rel=\"nofollow noopener\">https://neuroscienceblueprint.nih.gov/human-connectome/connectome-programs</a>) and other connectome work in the US and around the globe, such as the OpenWorm Project (<a href=\"http://openworm.org/\" rel=\"nofollow noopener\">http://openworm.org/</a>) and the Blue Brain Project (<a href=\"https://www.epfl.ch/research/domains/bluebrain/\" rel=\"nofollow noopener\">https://www.epfl.ch/research/domains/bluebrain/</a>).</p>\n<p>Best regards,</p>\n<p>Kevin</p>"
    ],
    "1045": [
        "<p>It occurred to me, as I was engaged in a task the other day, that there must be many tools in the HPC ecosystem whose sole purpose is to make HPC life easier.  I\u2019m thinking that Ask.CI presents an optimal space for community members to share this inventory; even 10 posts could result in a useful reference catalogue of tips and tricks to boost our efficiency!  If we stick to open-source software, maybe originating from github repository, we\u2019d optimize accessibility.</p>\n<p>Who wants to go first?</p>",
        "<p>This is a really good question! There are several tools that I\u2019ve found useful over the years, specifically working on a SLURM cluster.</p>\n<ul>\n<li>\n<a href=\"https://researchapps.github.io/job-maker/\" rel=\"nofollow noopener\">job-maker</a> is a small interface that a center can deploy, customized to their slurm.conf (and actually a user can generate it too because the slurm.conf has to be readable by all nodes, and is readable by the user) to generate submission scripts.</li>\n<li>\n<a href=\"https://vsoch.github.io/lessons/smanage/\" rel=\"nofollow noopener\">smanage.sh</a> is a small bash script that can help with managing job arrays (status, submitting, etc.) created by Eric Surface from Harvard.</li>\n<li>\n<a href=\"https://github.com/sqshq/sampler\" rel=\"nofollow noopener\">sampler</a> is a really fun way to generate some kind of dashboard to monitor things, etc.</li>\n<li>\n<a href=\"https://vsoch.github.io/watchme/\" rel=\"nofollow noopener\">watchme</a> has command line, python decorators, and general functions for monitoring or running tasks.</li>\n<li>\n<a href=\"https://github.com/singularityhub/singularity-compose-examples\" rel=\"nofollow noopener\">singularity-compose</a> would be a way to run container orchestration (more for services) if your center doesn\u2019t have Open OnDemand or similar, and of course Singularity containers are a big part of that!</li>\n</ul>\n<p>I wouldn\u2019t tend to use HPC and easy in the same sentence, but maybe the above can make it \u201cless painful.\u201d <img src=\"https://ask.cyberinfrastructure.org/images/emoji/twitter/stuck_out_tongue.png?v=9\" title=\":stuck_out_tongue:\" class=\"emoji\" alt=\":stuck_out_tongue:\"></p>",
        "<p>Hi,</p>\n<p>We wrote some great user tools for sites that running slurm:</p>\n<p>jobstats - make it easy for users to see status of jobs, and what resources were actually utilized from the resources requested. We use this to help users create more efficient jobs.<br>\n\u2013 <a href=\"https://github.com/nauhpc/jobstats\" rel=\"nofollow noopener\">https://github.com/nauhpc/jobstats</a></p>\n<p>doppler - complementary webapp to jobstats showing users, and account job efficiency/resource wastage<br>\n\u2013 <a href=\"https://github.com/nauhpc/doppler\" rel=\"nofollow noopener\">https://github.com/nauhpc/doppler</a></p>\n<p>Best,<br>\nChris</p>",
        "<p>We use several custom in house bash and Python scripts to make administration easier. For job management and reporting we use XDmod. And we are about to put into production the Open Ondemand portal.</p>",
        "<p>I would be very interested in working together to compile these results into a list organized by tool \u201cclass\u201d or \u201ctags\u201d somehow. I\u2019m not sure if that would be best as a spreadsheet, bullet list, or something else.</p>\n<p>Although I\u2019m new to HPC, here are some tools I use that <em>might</em> be relevant:</p>\n<ul>\n<li>logging/monitoring\n<ul>\n<li>collector: telegraf</li>\n<li>data store: graphite</li>\n<li>dashboard/visualization: grafana, jupyter, blogdown+hugo+Rmd</li>\n</ul>\n</li>\n<li>job orchestration    : airflow, luigi</li>\n<li>configuration management : puppet</li>\n</ul>",
        "<p>Is this question concerned more with user-end experience or admin side? For admin side, we can list:</p>\n<ul>\n<li>XALT</li>\n<li>REMORA</li>\n</ul>\n<p>For end-user side, Open OnDemand is a versatile tool.</p>",
        "<p>I agree that compiling the results listed here by tag would be a very useful way to represent the information. We can add a framework for this in the \u201cresources\u201d section that is in early stages of development on the <a href=\"https://necyberteam.org\" rel=\"nofollow noopener\">northeast cyberteam site.</a>.  Will update here when it is there.</p>"
    ],
    "1018": [
        "<p>Research computing is a rapidly growing and developing profession, critical to universities, laboratories and organizations working toward integrating high performance computing (and other compute technologies) with researcher needs. Besides working with existing resources, research computing professionals are often the innovators when new methods and new ideas are needed to find the way forward.</p>\n<p>It seems likely that groups of research computing professionals have formed in order to productively share experiences and theories, discuss plans and funding opportunities, engage in multi-institution projects; in short, to embrace all of the crucial elements that comprise a successful professional community.</p>\n<p>Thus my question; what are the conferences relevant to Research Computing?  Which ones are the big, all-encompassing events, which are more detailed, which are midsize?  Are there cross-discipline conferences, and if so, which ones, and what do they concern?  Might one gathering favor industry, another benefit academia?</p>\n<p>I look forward to discussion around conference opportunities!</p>",
        "<p>I believe the Practice and Experience in Advanced Research Computing (PEARC) conference that begins happens next week is probably the largest gathering of practitioners. There are a few virtual communities that you can participate in as well. One of them is CarCC which hosts monthly video seminars and has a discussion list.</p>\n<p><a href=\"https://www.pearc.org/\" class=\"onebox\" target=\"_blank\" rel=\"nofollow noopener\">https://www.pearc.org/</a><br>\n<a href=\"https://carcc.org/\" class=\"onebox\" target=\"_blank\" rel=\"nofollow noopener\">https://carcc.org/</a></p>",
        "<p>I started writing this here, and realized I had too much to say, and wrote it up more properly -&gt;</p>\n<p><a href=\"https://vsoch.github.io/2019/conferences/\" class=\"onebox\" target=\"_blank\" rel=\"nofollow noopener\">https://vsoch.github.io/2019/conferences/</a></p>\n<p>I\u2019ll leave the original text here for ease of reading.</p>\n<hr>\n<p>I have a different point of view around this whole conferences thing - I don\u2019t have a direct answer to the question about \u201cWhat conferences are great\u201d but rather what I hope for the future.</p>\n<p>First let\u2019s discuss the pros of going to a conference. As is stated in the question, there are numerous, from networking (career and growth opportunities) to discovering new ideas, to collaboration. But what about the cons? Well, conferences are:</p>\n<ul>\n<li>expensive, usually between $300-$600 for a plane ticket depending on where you live, always bordering around high hundreds for a few nights in a hotel, and then maybe a few hundred for the registration. For tech conferences the registration fees go over $1000. This means that even the cheapest conferences will set a group back maybe $300, more realistically it\u2019s around $1000.</li>\n<li>Sometimes passive. What I mean to say is that there are so many talks and events, it\u2019s easy to go and sit like a baked potato. I guess it\u2019s not so bad to sit and enjoy yourself, but there is opportunity cost of the time. Tickets aren\u2019t being answered, users helped, etc.</li>\n</ul>\n<p>The last point isn\u2019t really so important - everyone could deserve to take some time. The thing that is troubling is that there is a stark contrast between extra funds that a traditional research computing group has and the costs, period. Attending a conference is a privilege, because it means someone else can do the work for you, and (typically) your group can pay your way. The question I\u2019m interested in is Who do we <em>not</em> see at conferences? Who doesn\u2019t get the benefit of learning and networking? The answer are the groups that maybe need it most.</p>\n<p>I don\u2019t think it\u2019s fair to be critical without making suggestions for change, so now I\u2019ll share those thoughts. I also want to point out my bias - I\u2019m a remote worker for over 2 years and I\u2019ve run the gamut from attending conferences in person, to not being able to attend, to having quasi participation remotely. The change that I want to see is more initiative taken to make conferences \u201cRemote first.\u201d Here is how that would look:</p>\n<ul>\n<li>An agenda is made, just as it was before, for remote participation. For multiple events happening at once, you just have multiple call ins. A call in is akin to a room.</li>\n<li>For smaller group sessions, you have \u201cbreak out\u201d sessions on the phone. Something like zoom can do this for you. There is both audio / video and chat. I would go as far to say that it\u2019s a more comfortable environment (for some) to speak than raising a hand in a big conference hall.</li>\n</ul>\n<p>Now here are the benefits:</p>\n<ul>\n<li>\n<strong>Broad inclusion</strong>: It changes from an event of privilege to one of all inclusion. (Most) HPC and research computing folks have an internet connection and computer.</li>\n<li>\n<strong>Temporal Freedom</strong>: In terms of time, it\u2019s much easier to attend select sections without leaving the home base. The participants (if needed) can still provide support to their users without taking a week vacation. You save all the time that would incur from travel, taking cabs, etc.</li>\n<li>\n<strong>Global Participation</strong> if it\u2019s virtual, anybody can attend easily, even outside of the US, both as participants and speakers.</li>\n<li>\n<strong>Comfortable</strong> Not everyone is comfortable in the environment that a conference affords. The talks might be ok, but events can be loud, crowded, and overwhelming. With virtual participation, people can be comfortable.</li>\n</ul>\n<p>I think it\u2019s probably easy to get access to a lot of conferences, and not think twice about the people that cannot go. And the negative <a>effects on the environment</a>. Others are noticing, and the HPC world should too. Here are some examples of other initiatives that have taken notice:</p>\n<ul>\n<li><a href=\"https://www.fitz.cam.ac.uk/news/fitzwilliam-college-launches-carbon-offset-programme\" rel=\"nofollow noopener\">https://www.fitz.cam.ac.uk/news/fitzwilliam-college-launches-carbon-offset-programme</a></li>\n<li><a href=\"https://twitter.com/jayvanbavel/status/1148673258574815232\" rel=\"nofollow noopener\">Idea to combine smaller conferences to \u201cmini conferences\u201d</a></li>\n<li><a href=\"https://medium.com/sylabs/announcing-the-singularity-webinar-program-a1d38846ef53\" rel=\"nofollow noopener\">Sylabs is trying regular webinars</a></li>\n<li><a href=\"https://runningremote.com/remote-work-conference/\" rel=\"nofollow noopener\">Remote First Conference</a></li>\n</ul>\n<p>A lot of prominent scientists are also putting down their foot and saying \u201cI won\u2019t be traveling, but I\u2019ll give a talk remotely\u201d and (imho) that\u2019s a way to lead to change:</p>\n<ul>\n<li><a href=\"https://hannahknox.wordpress.com/2019/07/02/a-year-without-flying/\" rel=\"nofollow noopener\">example 0</a></li>\n<li>\n<a href=\"https://twitter.com/russpoldrack/status/1148290418804551684\" rel=\"nofollow noopener\">example 1</a> and <a href=\"http://www.russpoldrack.org/2019/06/why-i-will-be-flying-less.html\" rel=\"nofollow noopener\">write up</a>\n</li>\n</ul>\n<p>There is another tweet I can\u2019t find again where, instead of traveling, a scientist gave a remote talk, and when he finished, a bottle of expensive wine showed up at his door, which was <em>much</em> cheaper than having to fund his travel there.</p>\n<p>The components that we lose are the big social ones - grabbing lunch with colleagues, or hanging out. But maybe we don\u2019t have to - why not have sessions over zoom that are entirely social?<br>\nMy point isn\u2019t that conferences are bad, but the practice is dated, hurts the environment, and leaves a lot of folks out. I think it\u2019s probably time to try and do better, even if just starting small (making them remote friendly), and asking how we can do better.</p>",
        "<p>The hope for this topic was not necessarily to debate the pros and cons of conferences, but to bring forward the options as they exist today.  Certainly there is a place for discussing the merits of bringing people together in person, and I respect those opinions.  But regardless of how and where they happen, it would be good to know what meetings are taking place currently in the research computing world.  From local and regional to international.</p>\n<p>For example, RMACC, which happens in May in Boulder, CO; PEARC, occurring now in Chicago; SC, scheduled for November in Denver; are some that come to mind.  It would be especially helpful to hear of the smaller-scale opportunities.</p>",
        "<p>The president of M\u00e9xico, L\u00f3pez Obrador, who slashed funding for basic and applied research in M\u00e9xico on May 03, calls traveling to conferences \u2018political tourism\u2019 according to Science, vol 365, pages 301 and 305.</p>",
        "<p>I just attended IEEE HPEC conference, which is held in the Boston, Massachusetts area. It was less expensive compared to other conferences. While I\u2019m a system administrator, it was a good opportunity to hear about the different types of problems being solved with Research Computing. I made some good contacts and learned about this website!</p>\n<p>The focus of that conference seems to be secure hardware, FPGAs, GPUs, AI, etc. There was an education session that I was interested in since we are looking for ways to educate our system administrators and users alike on our systems.</p>"
    ],
    "1076": [
        "<p>I am creating a new survey to gather the cyberinfrastructure needs of our research community. If you use a similar survey, please share. Thank you in advance!</p>",
        "<p>I don\u2019t have a survey, but I did create a few weeks back a <a href=\"https://vsoch.github.io/resource-explorer/\" rel=\"nofollow noopener\">resource explorer</a> that (I think?) might help to narrow down some wanted features to a final set. You could modify that interface to be questions about needs, and then to submit a post to formspree to collect the data. The code is on GitHub, of course! <a href=\"https://github.com/vsoch/resource-explorer\" rel=\"nofollow noopener\">https://github.com/vsoch/resource-explorer</a></p>"
    ],
    "1064": [
        "<p>Let\u2019s re-imagine the Question of the Week (QoW)!  The motivation behind the promotion, which will remain the same, is to generate interest, discussion and ultimately grow the Ask.CI community in size, depth and buzz.  On a more targeted level, the QoW aimed to highlight questions still needing answers, and coerce the research computing experts we know are out there whose shared knowledge can benefit many others.  This goal is still important, and we wish to continue to encourage this, possibly along with offering ways to introduce some excitement (badges, pizza parties; you name it)!  The floor is open\u2026</p>",
        "<p>Okay - thinking from a completely idealistic standpoint here.</p>\n<ul>\n<li>\n<strong>Automated</strong> The current question of the week is painfully chosen - we spend oodles of time choosing, discussing, and then it comes down to an email and Twitter post that (for some weeks) doesn\u2019t get great response. Ideally, the question would be chosen not manually, but by way of some automated metrics that indicate \u201cthis question is highest interest to users\u201d or \u201cthis question could be highest interest but hasn\u2019t been seen yet.\u201d I note that this is idealistic because AskCI doesn\u2019t have the traffic or tooling to make this a reality.</li>\n<li>\n<strong>Targeted</strong> Currently, we are sort of all over the place in terms of the questions that we share. While targeting different groups is probably okay to do, we should be asking ourselves every week \u201cWho is this intended for?\u201d because I would suspect a large percentage of our questions we would want to target users and students, but a significant portion are intended for sysadmins and similar.</li>\n</ul>\n<p>Now for the re-imagining part. I always approach a QoW and ask myself \u201cWhy would I want to answer this?\u201d or for a different question - of the things that I do reply to, what is my incentive? For example, for this question I\u2019m answering right now, I am invested in the community and I want to make it better. That might hint to an issue we have with AskCI - we have category boards (locales) but not a strong sense of community tied to them. If we start with the Groups base, and then have answering questions somehow integrated, that might incentivize someone (that currently doesn\u2019t care) to care about the same questions. There are a few simple ideas that come to mind:</p>\n<ul>\n<li>calculate monthly metrics to award a winning team / group (I implemented this -&gt; <a href=\"https://vsoch.github.io/2019/discourse-rankings/\" rel=\"nofollow noopener\">https://vsoch.github.io/2019/discourse-rankings/</a>)</li>\n<li>Recruit locales to come up with a question they pose to <em>their</em> users, and then they award some winning answer (and given enough locales, this would ideally be a few times a year per group).</li>\n<li>Turn it into a game. Have a question of the week be a puzzle, a task, or some kind of challenge for groups (or individuals, if appropriate) to submit answers to. This might be more appropriate to give a longer time for (Question of the Month, Query of the Quarter, Task of the Trimester, etc.).</li>\n<li>Target specific experts. Don\u2019t have an agenda, but reach out to each expert and ask them the most interesting thing they learned in the last month / year (or in recent memory) and then to briefly write a paragraph. The content is a bit unpredictable, but I would say it would be easy to frame in a QoW sort of way.</li>\n<li>I\u2019m still gung-ho on some kind of annual / regular competition with a pizza party prize. Like, how can you not want that. <img src=\"https://ask.cyberinfrastructure.org/images/emoji/twitter/stuck_out_tongue.png?v=9\" title=\":stuck_out_tongue:\" class=\"emoji\" alt=\":stuck_out_tongue:\">\n</li>\n</ul>\n<p>I\u2019ll share more if I think of it!</p>"
    ],
    "953": [
        "<p>Hello!</p>\n<p>I was wondering, is there a good resource to learn how to get started with RoCE?</p>\n<p>It seems to me like RoCE is a cheaper way to get started with experimenting with high-performance networking, since you can use regular switches (instead of having to shell out the $$$ for Infiniband switches and cables).  I am wondering if there is a good article, that will take you from zero to \u2018hello world\u2019.</p>\n<p>I did find <a href=\"https://community.mellanox.com/s/article/howto-configure-soft-roce\" rel=\"nofollow noopener\">Mellanox\u2019s How To article</a> while searching around, but I think it might be out-of-date: It talks about building a kernel with the SoftRoCE modules and userspace programs, but looking at Ubuntu Bionic, it seems those are already available.</p>"
    ],
    "533": [
        "<p>Hi All,</p>\n<p>Is anyone using RAM disk(tmpfs) on your cluster queuing system to optimize file I/O ?</p>\n<p>Thanks,<br>\nSarvani Chadalapaka<br>\nHPC Administrator<br>\nUniversity of California Merced, Office of Information Technology<br>\nschadalapaka@ucmerced.edu| it.ucmerced.edu</p>",
        "<p>I\u2019m a little confused by the wording of your question, but I\u2019ll try to answer the best that I can. Typically the queueing system itself does not do enough file I/O to require an optimized file system so I would not suspect that most sites have the queueing system writing to a tmpfs file system. The user jobs that the queueing system runs may have many different file I/O patterns and a tmpfs could be useful there. At our site we do have a tmpfs filesystem that is mounted on all compute nodes as /dev/shm/ and is available for users to use in their jobs.</p>",
        "<p>Let me rephrase for schadalapaka here,</p>\n<p>We have a number of users that have a number of jobs using FileIO,  some of them are aware of /dev/shm, and (try to) use it properly. It is annoying as they have to 1) learn about it, 2) clean up after themselves.</p>\n<p>We saw that system.d is mounting /run/user/$(id -u $USER), and tear it down for interactive session, though Slurm job does not seem to trigger it.</p>\n<p>Do you have any experience in hooking something similar into a scheduler so that user have easy access and discoverability while still making sure resources get cleaned up?</p>",
        "<p>hmmm\u2026  If I\u2019m understanding you <a class=\"mention\" href=\"/u/mbussonnier\">@mbussonnier</a>  your looking for a simple way to make these accessible, and automate clean-up for reuse (1)? and you\u2019re stuck on the automated clean-up (2)?</p>\n<p>I believe using the Queue Schedulers Prolog &amp; Epilog  options might work for that.<br>\nThese are typically for set-up and tear-down that you want to keep separate from from the job.<br>\nIn most cases they can be set up at multiple \u2018independent\u2019 levels, including system (scheduler) and user/job as well as being able to separate batch verse interaction jobs.</p>\n<ul>\n<li>that should be enough to script-out your specifics and matching environment variables.</li>\n</ul>\n<p>Since you mentioned Slurm (gridEngine and PBS also have prolog/epilog configuration)<br>\n<a href=\"https://slurm.schedmd.com/prolog_epilog.html\" class=\"onebox\" target=\"_blank\">https://slurm.schedmd.com/prolog_epilog.html</a></p>\n<hr>\n<p>A follow up, some user will likely do things that fill the whole available space.<br>\nHas anyone had issues with this? How were they addressed?</p>\n<p>I haven\u2019t tried anything, but to my current thinking, having a separate mount to \u2018partition\u2019 off a limited fraction of the memory seems plausible?</p>",
        "<p>Many thanks, I\u2019ll have a look at that \u2013 relatively new to slurm; I\u2019ll see if I can integrate it to make the experience seemless and will try to report. I\u2019d still like to figure out how systemd is making that to not reinvent the wheel. I\u2019ve also encountered software this week-end that work on interactive and not on batch subscription because /run/user/$id was not available on batch jobs and $XDG_RUNTIME was set.</p>\n<p>I believe that /dev/shm and /run/usr/$id are limited by default to 1/2 the available ram, and at least /run/user/id is destroyed when a user has no more login session, so you are pretty much guarantied that the resources will be cleaned-up.</p>",
        "<p>It is configurable but I\u2019ve seen enough variations to take the position \u2018check the specifics for each setup\u2019</p>\n<p>/dev/shm is often half of RAM when it exist, but not /run/usr/$id (at least where I\u2019ve been)<br>\nFor example on a local workstation with a default install of CentOS 7 /run/usr/$id with one user is only 10% of RAM (3.2Gb of 32Gb) and on on some systems doesn\u2019t exist at at all.</p>\n<p>More importantly even assuming half RAM by default it\u2019s a potential issue when allocating individual cores you end up with a system reporting a total of <em>0.5 * core-count</em> of available RAM, leading to users having/causing issues with thrashing. \u2013 though user education seems the solution here.</p>\n<p>Much less of an issue if you\u2019re scheduling at the node level.</p>\n<p>&lt;But what do I know I\u2019m out of my league here\u2026&gt;</p>\n<hr>\n<p>Systemd - IDK, it might be calling systemd-tmpfiles<br>\n<code>systemd-tmpfiles (8)</code>, <code>tmpfiles.d (5)</code></p>",
        "<p>You could create a slurm epilog and prolog script to setup and clean the areas. Something like this (we do this for /tmp) \u2026</p>\n<p>prolog (task prolog):</p>\n<p>TMPDIR=\"/tmp/$SLURM_JOB_USER\"<br>\nJOBDIR=\"$TMPDIR/$SLURM_JOB_ID\"<br>\necho \u201cexport TMPDIR=$JOBDIR\u201d</p>\n<p>epilog:</p>\n<p>TMPDIR=\"/tmp/$SLURM_JOB_USER\"<br>\nJOBDIR=\"$TMPDIR/$SLURM_JOB_ID\"<br>\nTMP=$TMPDIR<br>\nTEMP=$TMPDIR<br>\nrm -rf $JOBDIR</p>"
    ],
    "1023": [
        "<p>Hi All,</p>\n<p>Our supercomputer at Brown has relied on TurboVNC for several years. Our on-prem VMs use FastX. We are evaluating whether we should move our HPC environment to FastX, Open OnDemand (or other?)\u2026 We are hoping to get some feedback from others (both system administrators or users) on what their institution uses and the pros and cons encountered along the way. What have been your experiences? Thanks!</p>",
        "<p>We use open On Demand, now not for one but <em>two</em> of our clusters! Our admins are fairly conservative about adding new tools so this speaks very highly for OnDemand, and (although I\u2019m not an admin) although there are various bugs along the way, overall it seems to be an overwhelmingly positive addition for users to have interactive notebooks and file interaction. My issue with OnDemand was that previously it was hard to develop, because the primary avenue was to have it set up on a cluster and create a development sandbox (that seems like high barrier to entry to me). To help I created an ood compose <a href=\"https://github.com/vsoch/ood-compose\" rel=\"nofollow noopener\">https://github.com/vsoch/ood-compose</a> set of containers that includes OOD with slurm, and my development itch is scratched. There are overall very good tutorials <a href=\"https://osc.github.io/ood-documentation/release-1.6/app-development.html\" rel=\"nofollow noopener\">https://osc.github.io/ood-documentation/release-1.6/app-development.html</a> and a responsive team, so that speaks more highly for the software.</p>\n<p>I can\u2019t comment on TurboVNC or FastX because I don\u2019t have much experience with them.</p>",
        "<p>Thanks! That\u2019s very helpful. Is Open OnDemand the only system in place for a graphical interface to those two clusters? or is it used in addition to something else?</p>\n<p>Thanks again!</p>",
        "<p>Before that we didn\u2019t have many options - it came down to the user starting a job that would then be port forwarded to his or her local machine. We later had a <a href=\"https://vsoch.github.io/lessons/sherlock-singularity/\" rel=\"nofollow noopener\">tool to help</a> and it would provide general job templates to submit the job, wait for the node to be available, and then do the forward. There used to be some command line tools to help, but it looks like our documentation has replaced that with what is provided by OnDemand. <a href=\"https://www.sherlock.stanford.edu/docs/user-guide/ondemand/#interactive-applications\" rel=\"nofollow noopener\">https://www.sherlock.stanford.edu/docs/user-guide/ondemand/#interactive-applications</a> I can ping my group if you want details about the other means - the other common use case is just requesting a node with x11 support, and working with a graphical interface directly from it, e.g., something like:</p>\n<pre><code class=\"lang-bash\">ssh -XY &lt;username&gt;@&lt;cluster&gt;\nsrun --time 12:0:0 --x11 --pty bash\n</code></pre>\n<p>There\u2019s also random software like <a href=\"https://stanford-rc.github.io/docs-farmshare/docs/mobaxterm\" rel=\"nofollow noopener\">MobaXTerm</a> for windows users, etc. I\u2019ll check with my group to see what I\u2019m missing!</p>",
        "<p>I know of several sites that were previously using FastX when they installed OnDemand - individuals from those sites are traveling right now so hopefully they will post soon. My understanding is feedback from one site was that the interactive desktop\u2019s responsiveness was comparable, and the other site is hosting OnDemand alongside FastX. You can add arbitrary (external) hyperlinks to the OnDemand navbar so OnDemand can act as that \u201csingle pane of glass\u201d for your cluster but still provide quick access to users wanting to use FastX.</p>\n<p>OnDemand uses NoVNC HTML5 web client for interactive desktops and proxies the TurboVNC server through <a href=\"https://github.com/novnc/websockify\" rel=\"nofollow noopener\">websockify</a> a Python proxy that turns that traffic into WebSocket traffic. For non-VNC like Jupyter and RStudio OnDemand just proxies requests directly to those servers running on the compute nodes.</p>\n<p>I would think that since FastX has a custom web assembly client that there would be situations where it is more performant than the TurboVNC+websockify+NoVNC solution, but I do not know enough about the differences between the protocol FastX uses and the VNC protocol and haven\u2019t used FastX myself.</p>",
        "<p>At IU, for about 5 years now, we\u2019ve been offering a VNC-based frontend for one of our clusters using Thinlinc (<a href=\"https://www.cendio.com/thinlinc/what-is-thinlinc\" rel=\"nofollow noopener\">https://www.cendio.com/thinlinc/what-is-thinlinc</a>). Purdue has been using Thinlinc as well and we\u2019ve collaborated on a PEARC19 paper (<a href=\"https://dl.acm.org/citation.cfm?id=3332206\" rel=\"nofollow noopener\">https://dl.acm.org/citation.cfm?id=3332206</a>), which details each of our implementations and motivations, support challenges, and also includes results from a user survey that we conducted together.</p>",
        "<p>It\u2019s nice to see a conversation about Open OnDemand here.</p>\n<p>For anyone looking for more information about Open OnDemand, our website is <a href=\"http://openondemand.org/\" rel=\"nofollow noopener\">http://openondemand.org/</a></p>\n<p>We also have a very active discussion board at <a href=\"https://discourse.osc.edu/c/open-ondemand\" rel=\"nofollow noopener\">https://discourse.osc.edu/c/open-ondemand</a></p>",
        "<p>Hi Isabel,</p>\n<p>We\u2019ve used FastX for many years on local and cloud based instances, initially setting it up on our new central cluster here at Caltech. It\u2019s currently being run in parallel with OnDemand but there are plans to deactivate it now as pretty much all of our users have switched over to OnDemand. (For cloud based stuff we use AWS Nice DCV/Apache Guacamole)</p>\n<p>The way I see it, where as FastX offered a simple to use and performant graphical  interface to our cluster it didn\u2019t really address the breadth of features and expandability that makes the OnDemand portal compelling for our users of all technical levels. (i.e. html file browser, slurm job manager, job composer, single console for everything, shib auth etc).</p>\n<p>Having said that, I believe FastX recently released a major new update so it\u2019s possible some of these features may of been included. I will also note that FastX does support clustered installations so users could easily continue a login based graphical session regardless where the initial session started. (Not sure if OnDemand supports this with our without additional glue)</p>\n<p>We are currently waiting for OnDemand to support launching graphical sessions on the actual On Demand login hosts in addition to compute nodes as we often have long queue times and users just need to run some simple graphical apps. We\u2019d rather not sacrifice a compute host for this task. (FastX supports running graphical sessions on the login host themselves).</p>\n<p>There\u2019s something to be said for the openness, amount of momentum , excellent documentation and community involvement around the OnDemand ecosystem that is hard to come by with many closed source products. If you have any specific questions feel free to PM me.</p>\n<p>John</p>"
    ],
    "961": [
        "<p>Hello all, does anyone have any experience they can share about doing automated testing of HPC clusters? As in, we would like to have a test suite that we can run on our cluster any time we make a configuration change or upgrade, in order to exercise the most commonly used software and commands.</p>\n<p>We\u2019d love to hear about other groups\u2019 experiences in this area.</p>",
        "<p>hey <a class=\"mention\" href=\"/u/mrich\">@mrich</a>! I can speak a little bit for some of our clusters - I don\u2019t think most of our clusters have tests set up akin to continuous integration with changes in some code, but rather we have continuous  monitoring to reflect the status of servers (up, down, usage, etc.) I\u2019ll ping other members of my group to see if we can provide more feedback, because automated testing of HPC is a cool idea!</p>",
        "<p>Thanks, that would be great if you could ask around. For now we are going with a simple shunit2 script that will run on a login node and just load our most popular modules and run \u201chello world\u201d scripts to make sure nothing is egregiously broken.</p>",
        "<p>It really depends on how a cluster is operated, so one size doesn\u2019t fit all but I\u2019ve found that if every time something breaks that if I roll the root cause analysis/fix into a health check (I use NHC with Slurm) and/or a test job I can submit, that over time a pretty good testing regimen is assembled that is not just a change-driven test but an ongoing monitoring system looking for known errors specific to the current environment. Once the health checks exist to define the \u201cknown good state\u201d then by redefining that state (changing expected kernel version, for instance) the health check also becomes a handy tool for doing rolling reboots for kernel updates, BIOS/firmware updates, etc., making it easy to drain a cluster node-by-node based on anything you can write a script to test for.</p>\n<p>For the unknown errors\u2026good luck predicting those. <a href=\"https://dilbert.com/search_results?terms=Unplanned%20Outages\" rel=\"nofollow noopener\">https://dilbert.com/search_results?terms=Unplanned%20Outages</a></p>\n<p>griznog</p>",
        "<p>We successfully are using <a href=\"https://cug.org/proceedings/cug2017_proceedings/includes/files/pap122s2-file1.pdf\" rel=\"nofollow noopener\">ReFrame testing framework</a> at OSC and are presenting our paper at PEARC19 <a href=\"https://pearc19.conference-program.com/presentation/?id=pap152&amp;sess=sess204\" rel=\"nofollow noopener\">https://pearc19.conference-program.com/presentation/?id=pap152&amp;sess=sess204</a> on our use of it.</p>\n<p>The idea is to write automated regression tests that submit batch jobs that load modules and possibly exercise the software to a degree that the output can be automatically tested and verified. <a href=\"https://dl.acm.org/citation.cfm?doid=3152493.3152555\" rel=\"nofollow noopener\">Testpilot</a> might be another option but I don\u2019t know if it is available for other centers or not.</p>",
        "<p>Here is our PEARC19 paper <a href=\"https://dl.acm.org/citation.cfm?id=3332219\" rel=\"nofollow noopener\">\u201cA Continuous Integration-Based Framework for Software Management\u201d</a> on alternative to EasyBuild and Spack where we use ReFrame test suite and Gitlab webhooks to automate the execution of those tests on our three clusters.</p>",
        "<p>Do you have a version (a direct link to pdf or one you can upload) that isn\u2019t behind a login / paywall?</p>",
        "<p>Oops, <a href=\"https://dl.acm.org/citation.cfm?id=3332219\" rel=\"nofollow noopener\">https://dl.acm.org/citation.cfm?id=3332219</a> is the link. I\u2019ll fix my previous comment with the correct link.</p>",
        "<p>Oh sorry, that link is now paywall-ed\u2026</p>"
    ],
    "1021": [
        "<p>I was watching a video about a HPC facility in Canada talk about their new storage solution.  They mentioned that checkpoints used to run in the hours, but with the new solution it takes 1/2 hour.  In the HPC world, what is a checkpoint? Just curious\u2026</p>",
        "<p>Can we get a link to the video? Normally, the most relevant checkpoint definition would be <a href=\"https://en.wikipedia.org/wiki/Application_checkpointing\" rel=\"nofollow noopener\">application checkpointing</a>, but those wouldn\u2019t typically take that much time (unless there was a truly massive amount of data required to restart the application from an intermediate state).</p>\n<p>Maybe they were talking about something more storage-specific, like snapshots or replication (snapshots should be practically instantaneous, replication time is based off of the amount of data to send, and the throughput of the storage and any connecting networks).</p>"
    ],
    "975": [
        "<p>I saw this recent post on Twitter about the end of the life of the cluster titan, and it made me a little sad:</p>\n<aside class=\"onebox twitterstatus\">\n  <header class=\"source\">\n      <a href=\"https://twitter.com/ornldirector/status/1145845569597972481?s=21\" target=\"_blank\" rel=\"nofollow noopener\">twitter.com</a>\n  </header>\n  <article class=\"onebox-body\">\n    <img src=\"https://ask.cyberinfrastructure.org/uploads/default/original/1X/dfbb8cc43257f78f14cff56af316b6a54a40520e.jpeg\" class=\"thumbnail onebox-avatar\" width=\"400\" height=\"400\">\n<h4>\n  <a href=\"https://twitter.com/ornldirector/status/1145845569597972481?s=21\" target=\"_blank\" rel=\"nofollow noopener\">\n    Thomas Zacharia (ORNLdirector)\n  </a>\n</h4>\n\n<div class=\"tweet\">Very proud of the tremendous impact <a href=\"https://twitter.com/search?q=%23supercomputer\" target=\"_blank\" rel=\"nofollow noopener\">#supercomputer</a> Titan has had on the scientific community. In seven years, it\u2019s provided more than 26 billion core hours of computing time to hundreds of research teams around the globe. <a target=\"_blank\" href=\"https://twitter.com/olcfgov/status/1145726938008510469\" rel=\"nofollow noopener\">twitter.com/olcfgov/status\u2026</a>\n</div>\n\n<div class=\"date\">\n  <a href=\"https://twitter.com/ornldirector/status/1145845569597972481?s=21\" target=\"_blank\" rel=\"nofollow noopener\">12:04 AM - 2 Jul 2019</a>\n    <span class=\"like\">\n      <svg viewbox=\"0 0 512 512\" width=\"14px\" height=\"16px\" aria-hidden=\"true\">\n        <path d=\"M462.3 62.6C407.5 15.9 326 24.3 275.7 76.2L256 96.5l-19.7-20.3C186.1 24.3 104.5 15.9 49.7 62.6c-62.8 53.6-66.1 149.8-9.9 207.9l193.5 199.8c12.5 12.9 32.8 12.9 45.3 0l193.5-199.8c56.3-58.1 53-154.3-9.8-207.9z\"></path>\n      </svg> 25\n    </span>\n    <span class=\"retweet\">\n      <svg viewbox=\"0 0 640 512\" width=\"14px\" height=\"16px\" aria-hidden=\"true\">\n        <path d=\"M629.657 343.598L528.971 444.284c-9.373 9.372-24.568 9.372-33.941 0L394.343 343.598c-9.373-9.373-9.373-24.569 0-33.941l10.823-10.823c9.562-9.562 25.133-9.34 34.419.492L480 342.118V160H292.451a24.005 24.005 0 0 1-16.971-7.029l-16-16C244.361 121.851 255.069 96 276.451 96H520c13.255 0 24 10.745 24 24v222.118l40.416-42.792c9.285-9.831 24.856-10.054 34.419-.492l10.823 10.823c9.372 9.372 9.372 24.569-.001 33.941zm-265.138 15.431A23.999 23.999 0 0 0 347.548 352H160V169.881l40.416 42.792c9.286 9.831 24.856 10.054 34.419.491l10.822-10.822c9.373-9.373 9.373-24.569 0-33.941L144.971 67.716c-9.373-9.373-24.569-9.373-33.941 0L10.343 168.402c-9.373 9.373-9.373 24.569 0 33.941l10.822 10.822c9.562 9.562 25.133 9.34 34.419-.491L96 169.881V392c0 13.255 10.745 24 24 24h243.549c21.382 0 32.09-25.851 16.971-40.971l-16.001-16z\"></path>\n      </svg> 8\n    </span>\n</div>\n\n  </article>\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>But it also prompts a lot of questions that would be interesting to discuss. For example, it\u2019s logical that servers and disks only have a certain life, and often are replaced well before the infrastructure that they serve is taken down. It\u2019s also the case that hardware changes so quickly that at some point, given that funding is sufficient, it might even be easier to build a new cluster then to try and restore an older one.  So with this in mind, I want to ask my question. What is the typical lifecycle of a research cluster? What are the factors that determine clusters that have shorter lives versus longer lives?  What are the challenges in maintaining an older cluster? A newer one?  It occurs to me that documentation bases might be really hard to maintain just based on the fact that they need to be totally redone for some new entity every 5 to 10 years.   It also seems likely that there might be some balance between providing the newest and trendiest that the user might want, and maintaining something that is stable and reliable. Now given some common lifecycle that you might have in mind, is there any potential future that would allow for change to be less frequent, and clusters to be more stable?  Will clusters ever be able to have longer lives?</p>\n<p>Looking forward to hearing what people think!</p>",
        "<p>It really does depend on the type of research and how well the OS and research code is supported.  Assuming the machine can still receive software and security updates, the lifespan can be extended as long as the data does not change.  One could theoretically run mathematical simulations that output kilobyte files on nearly any machine but accommodating 5 petabytes of radio telescope data would take some upgrades for most machines.  For most scientific purposes, the amount of data to process is ever increasing.  This takes more disk space, memory and network capacity.  There is also a bias in development towards writing code that is easier for the developer to understand than the machine.  This means that people are writing horribly inefficient code in MatLab as opposed to writing better code in a more efficient language like C.  The solution for most institutions just seems to be buy faster computers as opposed to fixing their code so it runs more efficiently.  There are things that can be done to extend clusters\u2019 lives but it takes knowing how to use them within the framework of the research you\u2019re doing.</p>",
        "<p>Just to note the historical event, they took a video of \u201cTitan\u2019s Last Gasp\u201d - as if they needed to make it any more sad than it already was!</p>\n<p><a href=\"https://www.linkedin.com/posts/buddy-bland-b6b1b0111_titan-ornl-olcf-activity-6563128357154762753-ce3m/\" class=\"onebox\" target=\"_blank\" rel=\"nofollow noopener\">https://www.linkedin.com/posts/buddy-bland-b6b1b0111_titan-ornl-olcf-activity-6563128357154762753-ce3m/</a></p>\n<p>In case the post goes away, here is the content:</p>\n<blockquote>\n<p>Today, August 2, 2019 at 1:03 p.m. Eastern Daylight Time, the Titan supercomputer at ORNL breathed its last gasp as it was powered off.  Titan first appeared on the TOP500 list as <span class=\"hashtag\">#1</span> in November 2012 and remained in the top 5 of the list for 5 years and the top 10 for 6 years.  Over the course of its 7 years of service, Titan provided nearly 27 billion processor-hours of compute time to 896 research projects using AMD 18,688 16-core Opteron processors and NVIDIA K20x GPUs.</p>\n</blockquote>\n<blockquote>\n<p>Titan was reincarnated from its predecessor, Jaguar by replacing the node boards but reusing the cabinets, cooling system, backplane, interconnect cables, and power supplies. It is survived by its successor system, Summit which has 4,608 IBM Power9 CPUs and 27,648 NVIDIA V100 GPUs and is currently the <span class=\"hashtag\">#1</span> system on the TOP500 list.</p>\n</blockquote>\n<p><img src=\"https://ask.cyberinfrastructure.org/images/emoji/twitter/sob.png?v=9\" title=\":sob:\" class=\"emoji\" alt=\":sob:\"></p>"
    ],
    "115": [
        "<p>What practices do groups adopt to ensure their systems are consistent with requirements for private and/or personally identifiable data?</p>",
        "<p>Stanford has very clear risk classifications for data:</p>\n<p><a href=\"https://uit.stanford.edu/guide/riskclassifications\" class=\"onebox\" target=\"_blank\" rel=\"nofollow noopener\">https://uit.stanford.edu/guide/riskclassifications</a></p>\n<p>And then \u201cMinSec\u201d (Minimum Security) standards required for each level.</p>\n<p><a href=\"https://uit.stanford.edu/guide/securitystandards\" class=\"onebox\" target=\"_blank\" rel=\"nofollow noopener\">https://uit.stanford.edu/guide/securitystandards</a></p>\n<p>So - for the most part it\u2019s determining the risk classification, and then following the standard. If there is some new data type it might go to a board / IRB for discussion of what constitutes anonymized. But it\u2019s hard, because at the end of the day it\u2019s still a group of people with various checklist, with some help from automated checking tools, if applicable. There is a nice little portal with all the guides if you are interested:</p>\n<p><a href=\"https://uit.stanford.edu/security/sysadmin\" class=\"onebox\" target=\"_blank\" rel=\"nofollow noopener\">https://uit.stanford.edu/security/sysadmin</a></p>"
    ],
    "932": [
        "<p>I\u2019m running a calculation that parallelizes well using both MPI and OpenMP.  While reading online, it occurred to me that I might gain speedup with a GPU node.  How can I estimate the potential benefit?  And is it possible to monitor GPU usage once it\u2019s been allocated?</p>",
        "<p>You\u2019ll have to optimize your code to take advantage of the GPU cores.  (exactly how will vary by language)  Monitoring will depend on what brand of GPU you have.  Take a look at this:<br>\n<a href=\"https://askubuntu.com/questions/387594/how-to-measure-gpu-usage\" rel=\"nofollow noopener\">https://askubuntu.com/questions/387594/how-to-measure-gpu-usage</a><br>\nHow much benefit will depend on how well the code is optimized to run on the core.  Best test would be to re-write it and try to run the same size sample and see how long it takes.</p>",
        "<p>GPUs are really good at SPMD algos - single process, multiple data, wherein you do the same operation on zillions of input data. Matrix math, image processing (kind of the same thing), some Machine learning, approaches, shift-add hash operations on multiple strings/reads as per some bioinfo approaches.  If you interrupt these operations with logic (if, while, comparisons, subroutine calls), the magnitude of improvement drops tremendously.  So you have to decide whether your input data and analytical flow maps well to GPUs.  In many cases, small amounts of an analytical flow can be tremendously improved, but optimization of 5% of your entire flow to zero only saves you 5%.  Beware premature optimization. <img src=\"https://ask.cyberinfrastructure.org/images/emoji/twitter/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"></p>\n<p>You can see what your Nvidia GPUs are doing with the nvidia-smi utility which will tell you various stats  about the state and activity of the GPU. I\u2019ve no idea how to do this with ATI cards.</p>\n<pre><code># nvidia-smi\nTue Jun 11 07:53:46 2019       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 384.66                 Driver Version: 384.66                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX 1080    Off  | 00000000:02:00.0 Off |                  N/A |\n| 27%   40C    P0    39W / 180W |      0MiB /  8114MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  GeForce GTX 1080    Off  | 00000000:03:00.0 Off |                  N/A |\n| 27%   41C    P0    39W / 180W |      0MiB /  8114MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   2  GeForce GTX 1080    Off  | 00000000:83:00.0 Off |                  N/A |\n| 27%   41C    P0    39W / 180W |      0MiB /  8114MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code></pre>\n<p>Note also that many schedulers (SGE and derivatives at least) have trouble scheduling jobs for individual GPUs and sophisticated operations with them - suspending, moving, checkpointing jobs involving GPUs.  If you\u2019re not using a cluster, then this will have no impact, but if your jobs are large enough that they might migrate to a cluster, then this is a consideration.<br>\nI\u2019ve seen some reports that overall, the total cost for GPUs  vs the speed-up is about the same as for CPUs, obviously with some exceptions that are exactly matched for GPUs.  The upside for buying CPUs is that they can be used for general purpose computing without any re-coding.</p>\n<p>The easy test is to profile your code, see where the highest utilization is, and see if that code is amenable to re-coding for a GPU.  If it is, get theyself a cheap gaming GPU and see how easy re-coding in CUDA is, and see what the overall speedup is.</p>",
        "<p>I\u2019ve only tested this on our Cluster (nvidia I think) but I added a gpu task to watchme:</p>\n<aside class=\"onebox whitelistedgeneric\">\n  <header class=\"source\">\n      <img src=\"https://vsoch.github.io/watchme/assets/img/favicon.ico\" class=\"site-icon\" width=\"16\" height=\"16\">\n      <a href=\"https://vsoch.github.io/watchme/watchers/gpu/\" target=\"_blank\" rel=\"nofollow noopener\">watchme</a>\n  </header>\n  <article class=\"onebox-body\">\n    \n\n<h3><a href=\"https://vsoch.github.io/watchme/watchers/gpu/\" target=\"_blank\" rel=\"nofollow noopener\">gpu</a></h3>\n\n<p>The gpu watcher will help to monitor gpu devices (memory, power, versions, limits). This watcher must obviously be run on a machine with one or more GPU devices.</p>\n\n\n  </article>\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>Scroll down the page to the \u201cRun on the Fly, Command Line\u201d section. If you are using Python, you can also try out the decorator. Open an issue if you run into any trouble!</p>"
    ],
    "996": [
        "<p>At UMass Amherst, we\u2019ve received a number of requests to activate Rclone for our <a href=\"http://Box.com\" rel=\"nofollow noopener\">Box.com</a> instance \u2013 Rclone is currently disabled for our <a href=\"http://Box.com\" rel=\"nofollow noopener\">Box.com</a> instance, but FTPS is enabled.  In a meeting on the topic, I was asked if there are any security risks associated with enabling Rclone integration, and whether there are any performance benefits to using the Rclone integration over an FTPS connection to Box. I didn\u2019t have a good answer at the time, and am wondering if anyone else has any insight on this.</p>",
        "<p>Could you better explain what you mean by \u201cactivate Rclone for our Box instance?\u201d Typically if a user has access to RClone, they can configure their own integration with Box (we even have a walkthrough with RClone and Box under review <a href=\"https://github.com/stanford-rc/www.sherlock.stanford.edu/pull/18/files#diff-678ebf6d2fbf5fcdf43cc8c4ba491293\" rel=\"nofollow noopener\">here</a>. Are you saying that the university / research computing group has a Box account, and users are asking for you to authenticate? Or is it possible to deploy your own <a href=\"http://Box.com\" rel=\"nofollow noopener\">Box.com</a>, and allowing integrations is something you have turned off?</p>",
        "<p>Our university has an enterprise subscription to <a href=\"http://Box.com\" rel=\"nofollow noopener\">Box.com</a>.  For our University\u2019s enterprise users, the Rclone integration is currently disabled.</p>",
        "<p>So, presumably, this means:</p>\n<ul>\n<li>You have Box configured with SSO via SAML or something like that \u2013 users must go via an institutional login portal to access their Box accounts online, or use a separate Box-specific password for FTPS.</li>\n<li>You either have disabled OAuth authentication token use completely, or haven\u2019t enabled it specifically for rclone?</li>\n</ul>\n<p>So, it looks like, if this works the way I think it does, that there are a few things I can think of that you\u2019d want to consider.</p>\n<ul>\n<li>OAuth credentials may last for significantly longer than ordinary SAML sessions. On the other hand, I guess your current FTPS access uses separate passwords, so that\u2019s a bit of a worry as well. <a href=\"https://developer.box.com/docs/faqs#section-how-does-token-expiration-work-\" rel=\"nofollow noopener\">It looks like Box OAuth credentials last 1 hour</a> but can be refreshed automatically indefinitely, which may be a concern \u2013 you may want to check whether there is a way for an admin to invalidate a user\u2019s OAuth token.</li>\n<li>OAuth credentials may be stored locally on disk: an attacker with local data access or superuser privileges may have unlimited access to a user\u2019s Box data as long as the credentials remain valid.</li>\n<li>rclone seems to have both a <a href=\"https://rclone.org/commands/rclone_serve/\" rel=\"nofollow noopener\">server feature</a> and <a href=\"https://rclone.org/rc/\" rel=\"nofollow noopener\">an experimental remote control feature</a>, that you may consider worrisome things to have users run in conjunction with this.</li>\n</ul>\n<p>From a performance point-of-view, I don\u2019t have any figures \u2013 I\u2019d assume it\u2019s roughly comparable given they\u2019re both using TLS. It may be a little easier to control the number of parallel transfers in rclone to get the most performance.</p>\n<p>On the plus side, it should only use port 443, which is a little easier to configure a firewall for than FTPS.</p>"
    ],
    "989": [
        "<p>Hi,</p>\n<p>I would like to create a docker container for OpenFOAM-1.6-ext. I couldn\u2019t find an image of OpenFOAM-1.6-ext on dockerhub so I decided to create a Dockerfile my own. A screenshot of my Dockerfile is attached. I followed the OpenFOAM installation steps listed: <a href=\"https://openfoamwiki.net/index.php/Installation/Linux/OpenFOAM-1.6-ext/Ubuntu#Ubuntu_16.04\" rel=\"nofollow noopener\">here</a>. After I issued the command: sudo docker build -t \u201copenfoam-1.6-ext:docker\u201d, I received an error message stating \u201ccp: cannot stat \u2018etc/prefs.sh-EXAMPLE\u2019: No such file or directory\u201d. Does anyone know what I did wrong? <div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ask.cyberinfrastructure.org/uploads/default/original/1X/dee116d7ccabe784760903ab252df08e062ead7c.png\" data-download-href=\"https://ask.cyberinfrastructure.org/uploads/default/dee116d7ccabe784760903ab252df08e062ead7c\" title=\"dockerfile.png\"><img src=\"https://ask.cyberinfrastructure.org/uploads/default/optimized/1X/dee116d7ccabe784760903ab252df08e062ead7c_2_690x398.png\" alt=\"dockerfile\" data-base62-sha1=\"vNGfuA9ORYE6u5Ic9cr4dMEqwJC\" width=\"690\" height=\"398\" srcset=\"https://ask.cyberinfrastructure.org/uploads/default/optimized/1X/dee116d7ccabe784760903ab252df08e062ead7c_2_690x398.png, https://ask.cyberinfrastructure.org/uploads/default/optimized/1X/dee116d7ccabe784760903ab252df08e062ead7c_2_1035x597.png 1.5x, https://ask.cyberinfrastructure.org/uploads/default/optimized/1X/dee116d7ccabe784760903ab252df08e062ead7c_2_1380x796.png 2x\" data-small-upload=\"https://ask.cyberinfrastructure.org/uploads/default/optimized/1X/dee116d7ccabe784760903ab252df08e062ead7c_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">dockerfile.png</span><span class=\"informations\">2052\u00d71184 385 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Thanks,<br>\nWensi</p>",
        "<p>Hey Wensi! I saw you post to the Singularity list. Did you see this post:</p>\n<aside class=\"quote\" data-post=\"1\" data-topic=\"644\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img alt=\"\" width=\"20\" height=\"20\" src=\"/user_avatar/ask.cyberinfrastructure.org/vsoch/40/112_2.png\" class=\"avatar\">\n    <a href=\"https://ask.cyberinfrastructure.org/t/how-do-i-run-paraview-or-openfoam-on-an-hpc-resource/644\">How do I run paraview or openfoam on an HPC resource?</a> <a class=\"badge-wrapper  bullet\" href=\"/c/q-a\"><span class=\"badge-category-bg\" style=\"background-color: #652D90;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for questions that fit into a traditional Question/Answer format where there is a \u201cbest\u201d answer.  The Answer may evolve/change over time, so voting on answers by the community is highly encouraged to ensure that the  current best answer rises to the top!\">Q&amp;A</span></a>\n  </div>\n  <blockquote>\n    I want to use <a href=\"https://www.paraview.org/\" rel=\"nofollow noopener\">Paraview</a> but the installation seems very complicated. Is there a quick or easy way to do it without asking my admin for a special installation?\n  </blockquote>\n</aside>\n\n<p>And it wasn\u2019t helpful for you? I would like to offer to take a look at your recipe - could you copy paste the actual code into a text code block so I can see?</p>",
        "<p>Ah, so to give you a quick example, I can confirm that the file in question does exist in the repository you are cloning:</p>\n<aside class=\"onebox githubblob\">\n  <header class=\"source\">\n      <a href=\"https://github.com/Unofficial-Extend-Project-Mirror/openfoam-extend-OpenFOAM-1.6-ext/blob/master/etc/prefs.sh-EXAMPLE\" target=\"_blank\" rel=\"nofollow noopener\">github.com</a>\n  </header>\n  <article class=\"onebox-body\">\n    <h4><a href=\"https://github.com/Unofficial-Extend-Project-Mirror/openfoam-extend-OpenFOAM-1.6-ext/blob/master/etc/prefs.sh-EXAMPLE\" target=\"_blank\" rel=\"nofollow noopener\">Unofficial-Extend-Project-Mirror/openfoam-extend-OpenFOAM-1.6-ext/blob/master/etc/prefs.sh-EXAMPLE</a></h4>\n<pre><code class=\"lang-sh-example\">#----------------------------------*-sh-*--------------------------------------\n# =========                 |\n# \\\\      /  F ield         | OpenFOAM: The Open Source CFD Toolbox\n#  \\\\    /   O peration     |\n#   \\\\  /    A nd           | Copyright (C) 2010-2010 OpenCFD Ltd.\n#    \\\\/     M anipulation  |\n#------------------------------------------------------------------------------\n# License\n#     This file is part of OpenFOAM.\n#\n#     OpenFOAM is free software: you can redistribute it and/or modify it\n#     under the terms of the GNU General Public License as published by\n#     the Free Software Foundation, either version 3 of the License, or\n#     (at your option) any later version.\n#\n#     OpenFOAM is distributed in the hope that it will be useful, but WITHOUT\n#     ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n#     FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n#     for more details.\n#\n</code></pre>\n\n  This file has been truncated. <a href=\"https://github.com/Unofficial-Extend-Project-Mirror/openfoam-extend-OpenFOAM-1.6-ext/blob/master/etc/prefs.sh-EXAMPLE\" target=\"_blank\" rel=\"nofollow noopener\">show original</a>\n\n  </article>\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>But since you are putting the command into different <code>RUN</code> sections, you are likely in the wrong working directory. Instead, you will want to put all of those RUN statements into one command, here is a start:</p>\n<pre><code class=\"lang-auto\">RUN git clone https://github.com/Unofficial-Extend-Project-Mirror/openfoam-extend-OpenFOAM-1.6-ext.git OpenFOAM-1.6-ext &amp;&amp; \\\n    cd OpenFOAM-1.6-ext &amp;&amp; \\\n    git checkout 1.6.1 -b 16ext &amp;&amp; \\\n    cp etc/prefs.sh-EXAMPLE etc/prefs.sh &amp;&amp; \\\n    # do this for the remainder of your lines.\n</code></pre>\n<p>If you wanted to do something like the above (with multiple RUN statements, each coinciding with a Docker layer which isn\u2019t best practice) You would want to clone the repository with one RUN, and then set the <code>WORKDIR</code> to be the repository.</p>\n<pre><code class=\"lang-auto\">RUN git clone https://github.com/Unofficial-Extend-Project-Mirror/openfoam-extend-OpenFOAM-1.6-ext.git OpenFOAM-1.6-ext\nWORKDIR OpenFOAM-1.6-ext\n</code></pre>\n<p>Does that make sense?</p>",
        "<p>Hi Vanessa!</p>\n<p>I read the post you sent regarding to running openfoam on an HPC resource; it was really helpful. Right now I\u2019m trying to create a docker image, push it to docker hub, and then convert it to singularity container. I\u2019ve tried your suggestion of putting all RUN statements into one command and played with WORKDIR a bit:</p>\n<pre><code class=\"lang-auto\">FROM ubuntu:18.04\nMAINTAINER Wensi Wu&lt;wensi.wu.w@gmail.com&gt;\n\nENV DEBIAN_FRONTEND=noninteractive\n\nWORKDIR /home/wensi\n\nRUN apt-get -q update &amp;&amp; \\\n    apt-get -q -y upgrade &amp;&amp; \\\n    apt-get -q -y install git flex rpm build-essential zlib1g-dev binutils-dev &amp;&amp; \\\n    apt-get -q -y install openmpi-bin libopenmpi-dev cmake paraview libstdc++5 libiberty-dev &amp;&amp; \\\n    apt-get -q update &amp;&amp; \\\n    mkdir OpenFOAM &amp;&amp; \\\n    cd OpenFOAM &amp;&amp; \\\n    git clone https://github.com/Unofficial-Extend-Project-Mirror/openfoam-extend-OpenFOAM-1.6-ext.git OpenFOAM-1.6-ext &amp;&amp; \\\n    export HOME=/home/wensi &amp;&amp; \\\n    cd OpenFOAM-1.6-ext &amp;&amp; \\\n    git checkout 1.6.1 -b 16ext &amp;&amp; \\\n    cp etc/prefs.sh-EXAMPLE etc/prefs.sh &amp;&amp; \\\n    echo \"export WM_COMPILER=Gcc54\" &gt;&gt; etc/prefs.sh &amp;&amp; \\\n    sed -i s/\"#export WM_MPLIB=SYSTEMOPENMPI\"/\"export WM_MPLIB=SYSTEMOPENMPI\"/g etc/prefs.sh &amp;&amp; \\\n    sed -i s/\"#export OPENMPI_DIR=path_to_system_installed_openmpi\"/\"export OPENMPI_DIR=\\/usr\"/g etc/prefs.sh &amp;&amp; \\\n    sed -i s/\"^#export OPENMPI_BIN_DIR\"/\"export OPENMPI_BIN_DIR\"/g etc/prefs.sh &amp;&amp; \\\n    sed -i s/\"#export PARAVIEW_SYSTEM=1\"/\"export PARAVIEW_SYSTEM=1\"/g etc/prefs.sh &amp;&amp; \\\n    sed -i s/\"#export PARAVIEW_DIR=path_to_system_installed_paraview\"/\"export PARAVIEW_DIR=\\/usr\"/g etc/prefs.sh &amp;&amp; \\\n    sed -i s/\"^#export PARAVIEW_BIN_DIR\"/\"export PARAVIEW_BIN_DIR\"/g etc/prefs.sh &amp;&amp; \\\n    echo \"export WM_NCOMPPROCS=4\" &gt;&gt; etc/prefs.sh &amp;&amp; \\\n    sed -i -e 's=\\.OpenFOAM=\\.foam=' bin/paraFoam &amp;&amp; \\\n    sed -i -e 's/^unset/export LC_ALL=C; unset/' bin/paraFoam &amp;&amp; \\\n    ln -s /usr/bin/make bin/gmake &amp;&amp; \\\n    sed -i -e 's=\\(U_WIN32\\)$=\\1 -DENGINE=g' applications/utilities/postProcessing/dataConversion/foamToTecplot360/tecio/tecsrc/Make/tecioOptions &amp;&amp; \\\n    sed -i -e 's=software.sandia.gov/~jakraft=downloads.sourceforge.net/project/openfoam-extend/foam-extend-3.0/ThirdParty=g' ThirdParty/AllMake.stage3 ThirdParty/rpmBuild/SPECS/mesquite-2.1.2.spec &amp;&amp; \\\n    find src applications -name \"*.L\" -type f | xargs sed -i -e 's=\\(YY\\_FLEX\\_SUBMINOR\\_VERSION\\)=YY_FLEX_MINOR_VERSION &lt; 6 \\&amp;\\&amp; \\1=' &amp;&amp; \\\n    cp -r wmake/rules/linux64Gcc46 wmake/rules/linux64Gcc54 &amp;&amp; \\\n    sed -i 's/^\\(c++FLAGS.*\\)/\\1 -fpermissive/g' wmake/rules/linux64Gcc54/c++ &amp;&amp; \\\n    . etc/bashrc &amp;&amp; \\\n    cd ThirdParty &amp;&amp; \\\n    sed -i 's=^( rpm_make -p cmake=#( rpm_make -p cmake=' AllMake.stage1 &amp;&amp; \\\n    sed -i -e 's/\\(\\+.*LIBS = \\)/\\1-Xlinker --add-needed -Xlinker --no-as-needed /' ./rpmBuild/SOURCES/ParMGridGen-1.0.patch &amp;&amp; \\\n    sed -i -e 's/\\(\\+.*LIBS = \\)/\\1-Xlinker --add-needed -Xlinker --no-as-needed /' ./rpmBuild/SOURCES/ParMetis-3.1.1.patch &amp;&amp; \\\n    sed -i -e 's=$(LDFLAGS)=$(LDFLAGS) -pthread=' ./rpmBuild/SOURCES/scotch-5.1.10b_patch_0 &amp;&amp; \\\n    ./AllMake &gt; log.AllMake 2&gt;&amp;1 &amp;&amp; \\\n    cd .. &amp;&amp; \\\n    . etc/bashrc &amp;&amp; \\\n    ./Allwmake &gt; log.AllwMake 2&gt;&amp;1\n</code></pre>\n<p>When I tried to build the docker file, it stalled at line \". etc/bashrc &amp;&amp; \"  before \"cd ThirdParty &amp;&amp; \". It seems like it was having trouble accessing to ThirdParty directory.</p>\n<p>Thank you so much for your help!</p>\n<p>Wensi</p>",
        "<p>I have OpenFOAM-1.6-ext installed and built on my ubuntu. Can I create a docker image for the already built OpenFOAM? Would that be easier?</p>",
        "<p>heya <a class=\"mention\" href=\"/u/ww382\">@ww382</a>! I think you are close! To answer your question, it\u2019s not really possible to just move an installation on your host into the container. Give me a few minutes with your recipe and I\u2019ll see if I can make a version for you that works.</p>",
        "<p>Okay, this seems to work to install in Docker. It\u2019s a weird package because it expects a very particular path structure (installing to the user\u2019s home) and that gets more weird with Docker having home as /root. This compiles and finishes ok, however if you need to bring it over to HPC and convert to Singularity you\u2019ll have trouble with everything being in root\u2019s home. If that happens, what we can do is give permissions to some openfoam user in the container, and then install in openfoam\u2019s home and make it accessible to others. For now, give this a go:</p>\n<pre><code class=\"lang-auto\">FROM ubuntu:18.04\n\n# docker build -t wensi/openfoam .\n\nLABEL MAINTAINER Wensi Wu&lt;wensi.wu.w@gmail.com&gt;\nENV DEBIAN_FRONTEND=noninteractive\n\nRUN apt-get -q update &amp;&amp; \\\n    apt-get -q -y upgrade &amp;&amp; \\\n    apt-get -q -y install git flex rpm build-essential zlib1g-dev binutils-dev &amp;&amp; \\\n    apt-get -q -y install openmpi-bin libopenmpi-dev cmake paraview libstdc++5 libiberty-dev &amp;&amp; \\\n    apt-get -q update\n\nWORKDIR /root\n\nRUN mkdir -p OpenFOAM &amp;&amp; \\\n    cd OpenFOAM &amp;&amp; \\\n    git clone https://github.com/Unofficial-Extend-Project-Mirror/openfoam-extend-OpenFOAM-1.6-ext.git OpenFOAM-1.6-ext\n\nWORKDIR /root/OpenFOAM/OpenFOAM-1.6-ext\n\nRUN git checkout 1.6.1 -b 16ext &amp;&amp; \\\n    cp etc/prefs.sh-EXAMPLE etc/prefs.sh &amp;&amp; \\\n    echo \"export WM_COMPILER=Gcc54\" &gt;&gt; etc/prefs.sh &amp;&amp; \\\n    sed -i s/\"#export WM_MPLIB=SYSTEMOPENMPI\"/\"export WM_MPLIB=SYSTEMOPENMPI\"/g etc/prefs.sh &amp;&amp; \\\n    sed -i s/\"#export OPENMPI_DIR=path_to_system_installed_openmpi\"/\"export OPENMPI_DIR=\\/usr\"/g etc/prefs.sh &amp;&amp; \\\n    sed -i s/\"^#export OPENMPI_BIN_DIR\"/\"export OPENMPI_BIN_DIR\"/g etc/prefs.sh &amp;&amp; \\\n    sed -i s/\"#export PARAVIEW_SYSTEM=1\"/\"export PARAVIEW_SYSTEM=1\"/g etc/prefs.sh &amp;&amp; \\\n    sed -i s/\"#export PARAVIEW_DIR=path_to_system_installed_paraview\"/\"export PARAVIEW_DIR=\\/usr\"/g etc/prefs.sh &amp;&amp; \\\n    sed -i s/\"^#export PARAVIEW_BIN_DIR\"/\"export PARAVIEW_BIN_DIR\"/g etc/prefs.sh &amp;&amp; \\\n    echo \"export WM_NCOMPPROCS=4\" &gt;&gt; etc/prefs.sh &amp;&amp; \\\n    sed -i -e 's=\\.OpenFOAM=\\.foam=' bin/paraFoam &amp;&amp; \\\n    sed -i -e 's/^unset/export LC_ALL=C; unset/' bin/paraFoam &amp;&amp; \\\n    ln -s /usr/bin/make bin/gmake &amp;&amp; \\\n    sed -i -e 's=\\(U_WIN32\\)$=\\1 -DENGINE=g' applications/utilities/postProcessing/dataConversion/foamToTecplot360/tecio/tecsrc/Make/tecioOptions &amp;&amp; \\\n    sed -i -e 's=software.sandia.gov/~jakraft=downloads.sourceforge.net/project/openfoam-extend/foam-extend-3.0/ThirdParty=g' ThirdParty/AllMake.stage3 ThirdParty/rpmBuild/SPECS/mesquite-2.1.2.spec &amp;&amp; \\\n    find src applications -name \"*.L\" -type f | xargs sed -i -e 's=\\(YY\\_FLEX\\_SUBMINOR\\_VERSION\\)=YY_FLEX_MINOR_VERSION &lt; 6 \\&amp;\\&amp; \\1=' &amp;&amp; \\\n    cp -r wmake/rules/linux64Gcc46 wmake/rules/linux64Gcc54 &amp;&amp; \\\n    sed -i 's/^\\(c++FLAGS.*\\)/\\1 -fpermissive/g' wmake/rules/linux64Gcc54/c++ &amp;&amp; \\\n    . etc/bashrc &amp;&amp; \\\n    cd ThirdParty &amp;&amp; \\\n    sed -i 's=^( rpm_make -p cmake=#( rpm_make -p cmake=' AllMake.stage1 &amp;&amp; \\\n    sed -i -e 's/\\(\\+.*LIBS = \\)/\\1-Xlinker --add-needed -Xlinker --no-as-needed /' ./rpmBuild/SOURCES/ParMGridGen-1.0.patch &amp;&amp; \\\n    sed -i -e 's/\\(\\+.*LIBS = \\)/\\1-Xlinker --add-needed -Xlinker --no-as-needed /' ./rpmBuild/SOURCES/ParMetis-3.1.1.patch &amp;&amp; \\\n    sed -i -e 's=$(LDFLAGS)=$(LDFLAGS) -pthread=' ./rpmBuild/SOURCES/scotch-5.1.10b_patch_0 &amp;&amp; \\\n    ./AllMake &gt; log.AllMake &amp;&amp; \\\n    cd .. &amp;&amp; \\\n    . etc/bashrc &amp;&amp; \\\n    ./Allwmake &gt; log.AllwMake\n</code></pre>\n<p>I removed the bit that silences the printing of the comple to stdout, and also If you want to make the container flexible to future version changes you can add:</p>\n<pre><code class=\"lang-auto\">ENV OPENFOAM_BRANCH=1.6.1\nENV OPENFOAM_VERSION=1.6-ext\n</code></pre>\n<p>And then replace those throughout the file where they are needed. It\u2019s still compiling for me locally so likely I\u2019ll be updating the recipe with a change to the path (and I want to test that it works too) but this should at least give you something to start with.</p>",
        "<p>Thank you so so so much <a class=\"mention\" href=\"/u/vsoch\">@vsoch</a>! <img src=\"https://ask.cyberinfrastructure.org/images/emoji/twitter/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"></p>",
        "<p>Sure thing! I had some trouble above with the install to root, so I\u2019m refactoring now to change to an openfoam user. If I have luck I\u2019ll share!</p>",
        "<p>I was getting error messages like:</p>\n<pre><code class=\"lang-auto\">/usr/bin/ld: cannot find -lfiniteVolume \n/usr/bin/ld: cannot find -ldynamicMesh \n/usr/bin/ld: cannot find -lmeshTools\n</code></pre>",
        "<p>Yes this looks like not having libraries on <code>LD_LIBRARY_PATH</code> or needing to run ldconfig - I also noticed we are using an 18.04 base and the instructions are for 16.04 so I\u2019m trying that too, along with installing as the openfoam user. Man this takes forever to compile!</p>",
        "<p>I just hit these too (installing as openfoam user):</p>\n<pre><code class=\"lang-bash\">/usr/bin/ld: cannot find -lincompressibleTransportModels\n/usr/bin/ld: cannot find -lincompressibleRASModels\n/usr/bin/ld: cannot find -lbasicThermophysicalModels\n/usr/bin/ld: cannot find -lcompressibleRASModels\n/usr/bin/ld: cannot find -lfiniteVolume\n/usr/bin/ld: cannot find -lmeshTools\n/usr/bin/ld: cannot find -lsampling\n</code></pre>\n<p>I think I just stumbled on what might be the issue - when I shelled in and looked in the ThirdParty folder, there were no RPMS in the source directory. If I ran the AllMake again, it doesn\u2019t issue an actual error, but it\u2019s riddled with \u201cCannot find xxxx.rpm.\u201d If we scroll up a little more, there is a message of missing wget <img src=\"https://ask.cyberinfrastructure.org/images/emoji/twitter/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"> So - I installed wget (as root) and now the ThirdParty libraries are actually compiling! I\u2019ll post an update if/when I get through this bug.</p>",
        "<p>okay I think it\u2019s good! I was able to run <code>icoFoam</code>, and although the help text appears different than in the interface, it seems to work. <a class=\"mention\" href=\"/u/ww382\">@ww382</a> you likely have experience using this and can give the final say - here is the recipe that worked for me (notice wget added to the top):</p>\n<pre><code class=\"lang-auto\">FROM ubuntu:16.04\n\n# docker build -t wensi/openfoam .\n\nLABEL MAINTAINER Wensi Wu&lt;wensi.wu.w@gmail.com&gt;\nENV DEBIAN_FRONTEND=noninteractive\nENV OPENFOAM_BRANCH=1.6.1\n\nRUN apt-get -q update &amp;&amp; \\\n    apt-get -q -y install git flex rpm build-essential zlib1g-dev binutils-dev &amp;&amp; \\\n    apt-get -q -y install openmpi-bin libopenmpi-dev cmake paraview libstdc++5 libiberty-dev wget\n\nRUN useradd -ms /bin/bash openfoam\nWORKDIR /home/openfoam\nUSER openfoam\n\nRUN mkdir -p OpenFOAM &amp;&amp; \\\n    cd OpenFOAM &amp;&amp; \\\n    git clone https://github.com/Unofficial-Extend-Project-Mirror/openfoam-extend-OpenFOAM-1.6-ext.git OpenFOAM-1.6-ext\n\nWORKDIR /home/openfoam/OpenFOAM/OpenFOAM-1.6-ext\n\nRUN git checkout 1.6.1 -b 16ext &amp;&amp; \\\n    cp etc/prefs.sh-EXAMPLE etc/prefs.sh &amp;&amp; \\\n    echo \"export WM_COMPILER=Gcc54\" &gt;&gt; etc/prefs.sh &amp;&amp; \\\n    sed -i s/\"#export WM_MPLIB=SYSTEMOPENMPI\"/\"export WM_MPLIB=SYSTEMOPENMPI\"/g etc/prefs.sh &amp;&amp; \\\n    sed -i s/\"#export OPENMPI_DIR=path_to_system_installed_openmpi\"/\"export OPENMPI_DIR=\\/usr\"/g etc/prefs.sh &amp;&amp; \\\n    sed -i s/\"^#export OPENMPI_BIN_DIR\"/\"export OPENMPI_BIN_DIR\"/g etc/prefs.sh &amp;&amp; \\\n    sed -i s/\"#export PARAVIEW_SYSTEM=1\"/\"export PARAVIEW_SYSTEM=1\"/g etc/prefs.sh &amp;&amp; \\\n    sed -i s/\"#export PARAVIEW_DIR=path_to_system_installed_paraview\"/\"export PARAVIEW_DIR=\\/usr\"/g etc/prefs.sh &amp;&amp; \\\n    sed -i s/\"^#export PARAVIEW_BIN_DIR\"/\"export PARAVIEW_BIN_DIR\"/g etc/prefs.sh &amp;&amp; \\\n    echo \"export WM_NCOMPPROCS=4\" &gt;&gt; etc/prefs.sh &amp;&amp; \\\n    sed -i -e 's=\\.OpenFOAM=\\.foam=' bin/paraFoam &amp;&amp; \\\n    sed -i -e 's/^unset/export LC_ALL=C; unset/' bin/paraFoam &amp;&amp; \\\n    ln -s /usr/bin/make bin/gmake &amp;&amp; \\\n    sed -i -e 's=\\(U_WIN32\\)$=\\1 -DENGINE=g' applications/utilities/postProcessing/dataConversion/foamToTecplot360/tecio/tecsrc/Make/tecioOptions &amp;&amp; \\\n    sed -i -e 's=software.sandia.gov/~jakraft=downloads.sourceforge.net/project/openfoam-extend/foam-extend-3.0/ThirdParty=g' ThirdParty/AllMake.stage3 ThirdParty/rpmBuild/SPECS/mesquite-2.1.2.spec &amp;&amp; \\\n    find src applications -name \"*.L\" -type f | xargs sed -i -e 's=\\(YY\\_FLEX\\_SUBMINOR\\_VERSION\\)=YY_FLEX_MINOR_VERSION &lt; 6 \\&amp;\\&amp; \\1=' &amp;&amp; \\\n    cp -r wmake/rules/linux64Gcc46 wmake/rules/linux64Gcc54 &amp;&amp; \\\n    sed -i 's/^\\(c++FLAGS.*\\)/\\1 -fpermissive/g' wmake/rules/linux64Gcc54/c++ &amp;&amp; \\\n    . etc/bashrc &amp;&amp; \\\n    cd ThirdParty &amp;&amp; \\\n    sed -i 's=^( rpm_make -p cmake=#( rpm_make -p cmake=' AllMake.stage1 &amp;&amp; \\\n    sed -i -e 's/\\(\\+.*LIBS = \\)/\\1-Xlinker --add-needed -Xlinker --no-as-needed /' ./rpmBuild/SOURCES/ParMGridGen-1.0.patch &amp;&amp; \\\n    sed -i -e 's/\\(\\+.*LIBS = \\)/\\1-Xlinker --add-needed -Xlinker --no-as-needed /' ./rpmBuild/SOURCES/ParMetis-3.1.1.patch &amp;&amp; \\\n    sed -i -e 's=$(LDFLAGS)=$(LDFLAGS) -pthread=' ./rpmBuild/SOURCES/scotch-5.1.10b_patch_0 &amp;&amp; \\\n    ./AllMake &gt; log.AllMake &amp;&amp; \\\n    cd .. &amp;&amp; \\\n    . etc/bashrc &amp;&amp; \\\n    ./Allwmake &gt; log.AllwMake &amp;&amp; \\\n    ./Allwmake\n</code></pre>\n<p>That was fun <img src=\"https://ask.cyberinfrastructure.org/images/emoji/twitter/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"></p>",
        "<p>Thanks, <a class=\"mention\" href=\"/u/vsoch\">@vsoch</a>! I\u2019ll give you an update! <img src=\"https://ask.cyberinfrastructure.org/images/emoji/twitter/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"></p>",
        "<p>I pull the openfoam image from Docker Hub</p>\n<pre><code class=\"lang-auto\">singularity pull --name OpenFOAM-1.6-ext.simg docker://ww382/openfoam\n</code></pre>\n<p>Then shell into the image</p>\n<pre><code class=\"lang-auto\">singularity shell OpenFOAM-1.6-ext.simg \n</code></pre>\n<p>I encountered some permission issues, so I created a recipe file for Singularity:</p>\n<pre><code class=\"lang-auto\">Bootstrap: docker\nFrom: docker://ww382/openfoam\n\n%environment\n  export SHELL=/bin/bash\n  export PATH=/home/openfoam/OpenFOAM/OpenFOAM-1.6-ext/ThirdParty/packages/scotch-5.1.10b/platforms/linux64Gcc54DPOpt/bin:/home/openfoam/OpenFOAM/OpenFOAM-1.6-ext/ThirdParty/packages/mesquite-2.1.2/platforms/linux64Gcc54DPOpt/bin:/usr/bin:/home/openfoam/OpenFOAM/openfoam-1.6-ext/applications/bin/linux64Gcc54DPOpt:/home/openfoam/OpenFOAM/site/1.6-ext/bin/linux64Gcc54DPOpt:/home/openfoam/OpenFOAM/OpenFOAM-1.6-ext/applications/bin/linux64Gcc54DPOpt:/home/openfoam/OpenFOAM/OpenFOAM-1.6-ext/wmake:/home/openfoam/OpenFOAM/OpenFOAM-1.6-ext/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\n  export LD_LIBRARY_PATH=/home/openfoam/OpenFOAM/OpenFOAM-1.6-ext/ThirdParty/packages/scotch-5.1.10b/platforms/linux64Gcc54DPOpt/lib:/home/openfoam/OpenFOAM/OpenFOAM-1.6-ext/ThirdParty/packages/ParMGridGen-1.0/platforms/linux64Gcc54DPOpt/lib:/home/openfoam/OpenFOAM/OpenFOAM-1.6-ext/ThirdParty/packages/ParMetis-3.1.1/platforms/linux64Gcc54DPOpt/lib:/home/openfoam/OpenFOAM/OpenFOAM-1.6-ext/ThirdParty/packages/metis-5.0pre2/platforms/linux64Gcc54DPOpt/lib:/home/openfoam/OpenFOAM/OpenFOAM-1.6-ext/ThirdParty/packages/mesquite-2.1.2/platforms/linux64Gcc54DPOpt/lib:/home/openfoam/OpenFOAM/OpenFOAM-1.6-ext/lib/linux64Gcc54DPOpt/openmpi-system:/usr/lib/x86_64-linux-gnu/openmpi/lib:/usr//lib:/home/openfoam/OpenFOAM/openfoam-1.6-ext/lib/linux64Gcc54DPOpt:/home/openfoam/OpenFOAM/site/1.6-ext/lib/linux64Gcc54DPOpt:/home/openfoam/OpenFOAM/OpenFOAM-1.6-ext/lib/linux64Gcc54DPOpt:/home/openfoam/OpenFOAM/OpenFOAM-1.6-ext/lib/linux64Gcc54DPOpt/dummy\n  export FOAM_APP=/home/openfoam/OpenFOAM/OpenFOAM-1.6-ext/applications\n  export FOAM_APPBIN=/home/openfoam/OpenFOAM/OpenFOAM-1.6-ext/applications/bin/linux64Gcc54DPOpt\n  export FOAM_INST_DIR=/home/openfoam/OpenFOAM\n  export FOAM_JOB_DIR=/home/openfoam/OpenFOAM/jobControl\n  export FOAM_LIBBIN=/home/openfoam/OpenFOAM/OpenFOAM-1.6-ext/lib/linux64Gcc54DPOpt\n  export FOAM_MPI=/home/openfoam/OpenFOAM/OpenFOAM-1.6-ext/lib/linux64Gcc54DPOpt/openmpi-system\n  export FOAM_RUN=/home/openfoam/OpenFOAM/user-1.6-ext/run\n  export FOAM_SOLVERS=/home/openfoam/OpenFOAM/OpenFOAM-1.6-ext/applications/solvers\n  export FOAM_SRC=/home/openfoam/OpenFOAM/OpenFOAM-1.6-ext/src\n  export FOAM_TUTORIALS=/home/openfoam/OpenFOAM/OpenFOAM-1.6-ext/tutorials\n  export FOAM_USER_APPBIN=/home/openfoam/OpenFOAM/user-1.6-ext/applications/bin/linux64Gcc54DPOpt\n  export FOAM_USER_LIBBIN=/home/openfoam/OpenFOAM/user-1.6-ext/lib/linux64Gcc54DPOpt\n  export FOAM_UTILITIES=/home/openfoam/OpenFOAM/OpenFOAM-1.6-ext/applications/utilities\n  export MPI_ARCH_PATH=/usr\n  export WM_DIR=/home/openfoam/OpenFOAM/OpenFOAM-1.6-ext/wmake\n  export WM_PROJECT_DIR=/home/openfoam/OpenFOAM/OpenFOAM-1.6-ext\n  export WM_PROJECT_INST_DIR=/home/openfoam/OpenFOAM\n  export WM_PROJECT_USER_DIR=/home/openfoam/OpenFOAM/user-1.6-ext\n  export WM_THIRD_PARTY_DIR=/home/openfoam/OpenFOAM/OpenFOAM-1.6-ext/ThirdParty\n\n%post\n  chmod -R 777 /home/openfoam/OpenFOAM\n</code></pre>\n<p>I then rebuilt the image</p>\n<pre><code class=\"lang-auto\">sudo singularity build OpenFOAM-1.6-ext.simg Singularity\n</code></pre>\n<p>The permission issues were resolved, but then I encountered error messages related storage space issue</p>\n<pre><code class=\"lang-auto\">cp: error writing 'damBreakWithObstacle/0/polyMesh/faces': No space left on device\ncp: error writing 'damBreakWithObstacle/0/polyMesh/neighbour': No space left on device\ncp: error writing 'damBreakWithObstacle/0/polyMesh/owner': No space left on device\n</code></pre>\n<p>I read that the solution to that is to set the <code>$SINGULARITY_CACHEDIR</code> to a different location. What are some typical locations to set the cache directory?</p>",
        "<p>Does the build fail when it\u2019s extracting layers, or when it\u2019s doing your customization? If the first,  then it would be fairly easy to use /tmp for the cache. Try:</p>\n<pre><code class=\"lang-bash\">mkdir -p /tmp/.singularity\nexport SINGULARITY_CACHEDIR=/tmp/.singularity\n</code></pre>\n<p>I\u2019m assuming you aren\u2019t running on an HPC resource given that you have sudo, but if you were doing a build of just the docker container on HPC (sudo not required) you could export to somewhere like $SCRATCH:</p>\n<pre><code class=\"lang-bash\">mkdir -p $SCRATCH/.singularity\nexport SINGULARITY_CACHEDIR=$SCRATCH/.singularity\n</code></pre>\n<p>That\u2019s actually been such a common recommendation that we have it done by default by our admins (and so you might ask your cluster admin to do the same!) I mention it because if you<br>\nare having trouble building locally, an option would be to do those additional customizations in the Docker container, push to something like <code>ww382/openfoam:singularity</code> and then pull directly onto the cluster, either of these would work:</p>\n<pre><code class=\"lang-bash\">$ singularity pull docker://ww382/openfoam:singularity\n$ singularity build docker://ww382/openfoam:singularity\n</code></pre>\n<p>So in summary:</p>\n<ol>\n<li>Try exporting the cache directory to /tmp/.singularity, or a filesystem with lots of space</li>\n<li>If you don\u2019t have enough on your local machine, do the changes in the Dockerfile, push to a different tag, and then pull directly.</li>\n</ol>",
        "<p>I was able to build successfully. The error occurred when I tried test run a tutorial case.</p>\n<p>I shell into the image:</p>\n<pre><code class=\"lang-auto\">singularity shell OpenFOAM-1.6-ext.simg \n</code></pre>\n<p>Then I sourced OpenFOAM bashrc:</p>\n<pre><code class=\"lang-auto\">source /home/openfoam/OpenFOAM/OpenFOAM-1.6-ext/etc/bashrc \n</code></pre>\n<p>I \u201ccd-ed\u201d into a tutorial case and executed the <code>Allrun</code> script:</p>\n<pre><code class=\"lang-auto\">cd /home/openfoam/OpenFOAM/OpenFOAM-1.6-ext/tutorials/multiphase/interDyMFoam/ras/damBreakWithObstacle/ &amp;&amp; \\\n./Allrun \n</code></pre>\n<p>Then I received the following errors:</p>\n<pre><code class=\"lang-auto\">cp: error writing '0/0-orig/U': No space left on device\ncp: error writing '0/0-orig/alpha1': No space left on device\ncp: error writing '0/0-orig/alpha1.org': No space left on device\ncp: error writing '0/0-orig/pd': No space left on device\n</code></pre>",
        "<p>This looks like it\u2019s not about the container, but the cluster / node where you are running it. I\u2019m not familiar with the tutorial, but I\u2019d take a look at the <code>Allrun</code> and determine where it\u2019s trying to write files (is it mounted on the host, or is it trying to write into a read only container?) and then assessing if you have space there.</p>",
        "<p>Also, if you are exporting all of those paths in the build recipe via the environment, what is gained from sourcing the bashrc?</p>",
        "<p>I haven\u2019t had a chance to work this out on a HPC yet because the system is down. In the meantime, I\u2019m trying to figure out all these on my local machine. Since I\u2019m running ubuntu on a virtual machine, could it be that the VM is somehow confused about where the execution is really occurring?</p>\n<p>That was a good point. I was exporting all of those paths in the build recipe because sourcing the bashrc within the container didn\u2019t seem to set up the paths for me. I now removed all of those paths in the recipe, build the image again, and sourcing the bashrc within the container. It seems like it\u2019s sourcing the bashrc script on my local directory rather than that within the container.</p>",
        "<p>Update: I created an overlay image and used it with the OpenFOAM container, the \u2018No space left on device\u2019 errors were gone! <img src=\"https://ask.cyberinfrastructure.org/images/emoji/twitter/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"></p>"
    ],
    "114": [
        "<p><strong>COMMENT</strong> This question may not have a single answer, but answers that highlight different possible ways to achieve good data transfer rates in different situations could all be useful.</p>\n<p>In all cases good transfer speeds will require that the end-point systems and the intermediate networks are configured appropriately. There is an extensive discussion of end-point configuration for fast data transfers here ( <a href=\"https://fasterdata.es.net/host-tuning/\" rel=\"nofollow noopener\">https://fasterdata.es.net/host-tuning/</a> ).</p>\n<p><strong>CURATOR:</strong> Chris Hill</p>",
        "<p>One option is bbftp (<a href=\"http://software.in2p3.fr/bbftp/\" rel=\"nofollow noopener\">http://software.in2p3.fr/bbftp/</a>). It is used by a number of NASA locations. A typical bbftp transfer command requires software to be installed at both the sending and receiving location. The software can be installed by an end user. A typical bbftp command is</p>\n<pre><code class=\"lang-auto\">bbftp -s -E PATH-TO-BBFTPD-ON-REMOTE -R bbftprc -V -p 8 -r 5 -u USER -i LIST-OF-COMMANDS REMOTE-MACHINE\n</code></pre>\n<p>where</p>\n<ul>\n<li>\n<code>PATH-TO-BBFTPD-ON-REMOTE</code>: is the location on the remote system of the bbftpd server command.</li>\n<li>\n<code>USER</code>: is the user name on the remote system</li>\n<li>\n<code>REMOTE-MACHINE</code>: is the remote system to transfer files to</li>\n<li>\n<code>LIST-OF-COMMANDS</code>: is a file with a list of commands to perform a transfer e.g.</li>\n</ul>\n<p>A typical <code>LIST-OF-COMMANDS</code> file contains entries like those shown here</p>\n<pre><code class=\"lang-auto\">setoption keepaccess\nsetoption keepmode\nsetoption nocreatedir\nput /nobackupp8/cnhill1/hawaii_npac/0001171008_V_10800.8150.1_1080.3720.90 /nfs/cnhlab003/cnh/llc4320/incoming/hawaii_npac/\nsetoption keepaccess OK\nsetoption keepmode OK\nsetoption nocreatedir OK\n</code></pre>",
        "<p>grid-ftp is another option</p>",
        "<p>globus is another option\u2026</p>",
        "<p>It is possible to script bulk uploads to facilities like Dropbox too. A set of scripts such as the ones here ( <a href=\"https://github.com/cpausmit/PyCox\" rel=\"nofollow noopener\">https://github.com/cpausmit/PyCox</a> ) can be used to make large scale transfers from multiple end-points to achieve good throughput.</p>",
        "<p>Aspera (<a href=\"http://asperasoft.com/software/\" rel=\"nofollow noopener\">http://asperasoft.com/software/</a>) is another option. It is a commercial too, but is widely used in the bioinformatics community.</p>",
        "<p>Amazon S3 provides another useful solution for intermediate staging of large files. The<br>\narticle here ( <a href=\"https://arxiv.org/abs/1708.00544\" rel=\"nofollow noopener\">https://arxiv.org/abs/1708.00544</a> ) provides a description of the mechanisms<br>\nand experience doing this.</p>",
        "<p>Besides the software solutions to this problem, there are other means to achieve good data transfer rates for bulk transfers. It can be cheaper and faster to use the \u201cFedEx protocol\u201d. Take a bunch of disks and mail them. The latency is not so great, but you can\u2019t beat the bandwidth.</p>",
        "<p><a href=\"http://moo.nac.uci.edu/~hjm/HOWTO_move_data.html\" rel=\"nofollow noopener\">http://moo.nac.uci.edu/~hjm/HOWTO_move_data.html</a> maybe?</p>"
    ],
    "792": [
        "<p>Anybody have experience with <strong>package managers</strong> that can provide user-managed software stack environment for HPC or similar kind of environment (multi-user, multi-node)? Another scenario is for users given one or more VM\u2019s where he/she could not have root access for any reason. Here are key requirements:</p>\n<ul>\n<li>This needs to be an environment that can be 100% controlled by the user, not by sysadmin.</li>\n<li>It needs to be a \u201cpackage manager\u201d (like apt, yum, fink, brew, pacman) with (mostly) simple one-command installation.</li>\n<li>It needs to have a gentle learning curve for the user (who is also the \u201cadmin\u201d of the software stack)</li>\n<li>The tool must be able to install to a user-specified location and <em>NOT</em> require superuser privileges (sudo/su) in the setup/build/install/config processes.</li>\n</ul>\n<p>It can have a GUI, but CLI capabilities is much preferred.</p>\n<p>Background: Under certain circumstances, user may choose to create his/her own software stack. Is there a platform that can be recommended for this user? The user has some basic skill on software install, build, etc but not too high skill in terms of software troubleshooting, etc. Ideally it contains many tested recipes for up-to-date software versions so the work on the user part is not too much.</p>\n<p>I notice there are software distribution with similar philosophy. For example, Conda/Anaconda for Python environment. Then there is also MSYS2, which seems to leverage ArchLinux pacman (?). And cygwin. But for Linux environment, anything like this? I came across Linuxbrew recently. Any input on your experience with this software management tool? How will this compare to Spack, etc?</p>\n<p>Wirawan</p>\n<p><em>(Question clarified 2019-03-06 because the original wording lacked \u201cpackage manager\u201d which can then be interpreted in many ways.)</em></p>",
        "<p><a href=\"https://conda.io/projects/conda/en/latest/user-guide/getting-started.html\" rel=\"nofollow noopener\">Conda</a> provides for much more than just python packages. I\u2019ve used <a href=\"https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html\" rel=\"nofollow noopener\">conda environments</a> (with <a href=\"https://conda.io/en/latest/miniconda.html\" rel=\"nofollow noopener\">miniconda</a> specifically) to create custom software stacks for various projects across different environments, including multiple HPC clusters. Adding channels such as <a href=\"https://conda-forge.org/\" rel=\"nofollow noopener\">conda-forge</a> and <a href=\"https://bioconda.github.io/\" rel=\"nofollow noopener\">bioconda</a> provides access to a wide variety of packages. And building your own isn\u2019t too hard.</p>",
        "<p>Conda, interestingly, is unable to unload the $PATH variable on deactivate, which makes it a bit inconvenient to switch between environments like modules or lmod does.</p>\n<aside class=\"onebox githubissue\">\n  <header class=\"source\">\n      <a href=\"https://github.com/conda/conda/issues/8070\" target=\"_blank\" rel=\"nofollow noopener\">github.com/conda/conda</a>\n  </header>\n  <article class=\"onebox-body\">\n    <a href=\"https://github.com/beenje\" rel=\"nofollow noopener\">\n<img src=\"https://ask.cyberinfrastructure.org/uploads/default/original/1X/6a0dfd8020bebb702254e321bbeb3aa7fe0293fd.jpeg\" class=\"thumbnail onebox-avatar\" width=\"96\" height=\"96\">\n</a>\n\n<h4><a href=\"https://github.com/conda/conda/issues/8070\" target=\"_blank\" rel=\"nofollow noopener\">Issue: Always restore PATH after deactivate even if modified in activate scripts</a></h4>\n\n<div class=\"date\" style=\"margin-top:10px;\">\n\t<div class=\"user\" style=\"margin-top:10px;\">\n\topened by <a href=\"https://github.com/beenje\" target=\"_blank\" rel=\"nofollow noopener\">beenje</a>\n\ton <a href=\"https://github.com/conda/conda/issues/8070\" target=\"_blank\" rel=\"nofollow noopener\">2019-01-09</a>\n\t</div>\n\t<div class=\"user\">\n\tclosed by <a href=\"https://github.com/beenje\" target=\"_blank\" rel=\"nofollow noopener\">beenje</a>\n\ton <a href=\"https://github.com/conda/conda/issues/8070\" target=\"_blank\" rel=\"nofollow noopener\">2019-01-31</a>\n\t</div>\n</div>\n\n<pre class=\"content\" style=\"white-space: pre-wrap;\">It's currently not possible to modify the PATH in the deactivate scripts. See #3915\nThere is one pending PR to fix that...</pre>\n\n<div class=\"labels\">\n</div>\n\n  </article>\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n  <div style=\"clear: both\"></div>\n</aside>\n",
        "<blockquote>\n<p>\u201cBuilding your own [package spec] isn\u2019t too hard\u201d</p>\n</blockquote>\n<p>This was what I was trying to avoid, actually. I added a clarifying stmt in the original question, i.e. what packaging system can be easily deployed by end-users in shared HPC or VM environment? Many HPC/VM users probably don\u2019t have the patience to build a \u201cpackaging spec\u201d file (be it for Conda, for LinuxBrew, for Debian, etc.).</p>",
        "<p><a class=\"mention\" href=\"/u/lparsons\">@lparsons</a> Can you say more about the packages you used in the past? Does Conda have packages for general (non-python) computing? Say, gcc, gdb, MPI, MKL or OpenBLAS, \u2026?</p>",
        "<blockquote>\n<p>Can you say more about the packages you used in the past? Does Conda have packages for general (non-python) computing? Say, gcc, gdb, MPI, MKL or OpenBLAS, \u2026?</p>\n</blockquote>\n<p>I\u2019ve used quite a few bioinformatics packages from <a href=\"https://bioconda.github.io/\" rel=\"nofollow noopener\">bioconda</a>, including samtools, STAR, featureCounts, DESeq2, etc. In addition to the biology related bioconda channel, there is an extremely useful channel called <a href=\"https://conda-forge.org/\" rel=\"nofollow noopener\">conda-forge</a> that is a community driven effort to provide conda recipes for all sorts of general purpose computing tools, including things like <a href=\"https://github.com/conda-forge/openblas-feedstock\" rel=\"nofollow noopener\">OpenBLAS</a>. Finally, there are many other channels, including the <a href=\"https://anaconda.org/anaconda\" rel=\"nofollow noopener\">Anaconda channel</a> that have many other things available.</p>",
        "<p>Users can always install there own packages and use something like <a href=\"http://modules.sf.net\" rel=\"nofollow noopener\">modules.sf.net</a> or lmod ( <a href=\"https://www.tacc.utexas.edu/research-development/tacc-projects/lmod\" rel=\"nofollow noopener\">https://www.tacc.utexas.edu/research-development/tacc-projects/lmod</a> ) to control their environment.  Often this is provided by the site, and users can create private modules so they don\u2019t have to setup everything from scratch.</p>\n<p>To wrap everything in a conda like environment and get modules you can use something like spack ( <a href=\"https://spack.readthedocs.io/en/latest/\" rel=\"nofollow noopener\">https://spack.readthedocs.io/en/latest/</a> )  Look at how I used it to bootstrap an environment quickly to do some cloud bench-marking to get an idea to get started (update to your own spack or master tree)  <a href=\"https://github.com/brockpalen/benchmark/blob/master/setup.sh\" rel=\"nofollow noopener\">https://github.com/brockpalen/benchmark/blob/master/setup.sh</a></p>\n<p>Spack will then setup most things for you, and then you just need to source the setup-env.sh to get everything going again.</p>",
        "<p>It\u2019s not quite what you\u2019re asking for, but you can give a non-root user access to Docker and users can then use Docker to install whatever packages are desired. A Berkeley data science class that I worked with has students install <a href=\"https://ipython.org/notebook.html\" rel=\"nofollow noopener\">Jupyter</a> to provide a Python environment. Then whatever packages are needed can be installed safely in the container.</p>",
        "<p><a>Spack</a>.</p>\n<p>Spack is so great that when I found out about it I teared up.</p>\n<p>For the installer:  get the code from github, source a script, type \u2018spack install package\u2019.  To use, \u2018spack load package\u2019.</p>\n<p>All installs are self-contained, so if you upgrade the HDF5 it won\u2019t break the IOAPI, NetCDF, CMAQ - whatever - in some other install.</p>\n<p>It has all the major bioinformatics packages including all the dependencies in the snp-pipeline (just an example of something with like 12 dependencies.)</p>\n<p>You can do permutations of libraries and compilers, although using something other than gcc is less magical.  I also had a problem with something that needed a gcc higher than our system gcc.</p>\n<p>You should modify the config files to use system openssl  This is easy.</p>",
        "<p>You can give a \u201cnon-root user\u201d access to run Docker, but that user can easily become root from within that container, as well as control a root running daemon process which lacks a reasonable control plane to manage security ACLs,\u2026 this may open up security concerns on many shared systems.</p>",
        "<p>Singularity is great for this. We use singularity to serve CernVM-FS, so that we don\u2019t have to install cvmfs on all of our compute nodes. Github integrates nicely with <a href=\"http://singularity-hub.org\" rel=\"nofollow noopener\">singularity-hub.org</a>, but users can also run docker images. Singularity only requires root privileges when trying to build an image, so this must be done on a local machine or via github + singularity (or gitlab + local CI/CD).</p>",
        "<p>Welcome to AskCI <a class=\"mention\" href=\"/u/drwx\">@drwx</a>!</p>\n<p><a class=\"mention\" href=\"/u/wirawan0\">@wirawan0</a> if you are avoiding using containers (if not, Singularity is a nice solution) out of spack, easybuild, and conda, I\u2019d recommend trying out spack. It will handle installing mostly everything (Python, Cmake, Autotools, etc.) packages, and if there is a package missing it\u2019s relatively easy to contribute. Heck, you can even install Singularity with spack (but the user wouldn\u2019t be able to run the final command that needs sudo to change permissions). If you want to try it out and have questions, or some request for packages missing, please ping me on GitHub (<a class=\"mention\" href=\"/u/vsoch\">@vsoch</a>) and I\u2019d be happy to help!</p>"
    ],
    "254": [
        "<p>I\u2019ve recently added an XSEDE portal user account for my institution.  As an administrator within my institution, are there any guidelines and/or restrictions I should be giving the users under my management in monitoring resource allocation session per user?</p>"
    ],
    "88": [
        "<p>Is there a general rule for when to consider parallelizing a job?  Such as, the amount of data to be processed, or the amount of time it takes to run in serial?  Or do other criteria take precedence; even though a calculation takes a really long time to run on one node, does that necessarily mean it will benefit from parallelization?</p>",
        "<p>Probably the most important criterion to consider when deciding whether or not to run a job in parallel is the ease with which the code can be parallelized.  Some codes are not conducive to parallelization; for example, it relies on dependencies throughout) and are better left to run in serial.  But, if the code has sections that can be run at the same time on different nodes (as in, these sections do not have interdependencies and can independently compute a value that can be shared later in the code), it\u2019s probably a good idea to restructure the code in parallel.  MPI (Message Passing Interface) is the prevalent mechanism with which to parallelize an HPC code.  As its name suggests, it enables sharing of information among compute nodes (and dispersal of this information to places in the code that require it).  Since more than one value can be calculated simultaneously, in theory the computation should complete in less time than it would if kept in serial form.</p>"
    ],
    "920": [
        "<p>I\u2019m working with a colleague at another institution to set up a Hadoop installation on their cluster. Since my experience is primarily with our Hadoop cluster, we\u2019d like to gather some info about how other campuses are using Hadoop. Can you please take a few minutes to answer this three question survey? You can either answer here on Ask.CI or in this <a href=\"https://forms.gle/483E1B8CV98iHb8Q7\" rel=\"nofollow noopener\">google form</a>.</p>\n<p>Do your researchers have access to an installation of Hadoop on your local cluster?<br>\nWhat is the level of usage?<br>\nWhich research groups dominate Hadoop usage at your institution?</p>",
        "<p>We previously had a Hadoop cluster with dedicated Hadoop nodes, but usage was very low, so those resources were repurposed for general use.  But we retain the ability to stand up a dedicated Hadoop cluster in case there is demand from researchers.</p>\n<p>Historically, our researchers that have expressed interest in Hadoop have generally been from bioinformatics, biostatistics, economics, or finance.  In many cases it was easy to create a simple proof of concept example, but very often Hadoop turns out to not be the best tool for the job.  Having to fit things into a map-shuffle-reduce paradigm, as well as the need to move data in and out of HDFS was often awkward enough to decide not to use Hadoop.</p>\n<p>But that was a few years ago, and the Hadoop ecosystem has grown since then.  We\u2019ve actually had some success with using a part of that ecosystem, Spark, in working with some of our finance users for certain types of analysis.  We like Spark in that it doesn\u2019t require your data to be loaded in HDFS.  Spark can just work with data that is already sitting on the filesystem, and there is no need to create a dedicated Hadoop cluster, so Spark jobs can run as just another type of parallel processing job, like MPI jobs.</p>"
    ],
    "105": [
        "<p>Research groups often have data that is in some way sensitive, for example, data containing gambling information, data containing educational interventions (concerns minors), or medical data. For example a state agency may contract a research group to carry on some analyses on some regulated activity (e.g. gambling). Even when de-identified (i.e. each person assigned a unique random identifier by the regulating agency), such data is sensitive and needs to be protected, both in transit and at rest. Such data can also be very large - in excess of 1Tb - concerning millions of subjects and billions of events, so the analysis may not fit in a single machine.</p>\n<p>So here are some more concrete questions:</p>\n<ol>\n<li>What software do you use to encrypt data?  Pros and cons of each package? Availability, cost, ease of use, compatibility with analyzing software?</li>\n<li>Are there encrypted solutions on the hardware level? Pros and cons of hardware vs. software encryption? Cost, speed, etc?</li>\n</ol>\n<p>Curator:Kristina Plazonic</p>",
        "<p>I highly recommend <a href=\"http://orcid.org/0000-0002-0139-7025\" rel=\"nofollow noopener\">Jonathan Crabtree</a>\u2019s talk \u201cImpact: Infrastructure for Privacy-Assured Computations\u201d talk given on 2019-04-23 as part of the Topics in Research Data Management webinar series hosted by Texas Digital Library. Here\u2019s the YouTube link, which is also posted at <a href=\"https://www.tdl.org/2018/10/texas-data-repository-webinar-series/\" rel=\"nofollow noopener\">https://www.tdl.org/2018/10/texas-data-repository-webinar-series/</a></p>\n<div class=\"lazyYT\" data-youtube-id=\"MKsrsV6KWsQ\" data-youtube-title=\"Topics in Research Data Management #4: Jonathan Crabtree, The Odum Institute\" data-width=\"480\" data-height=\"270\" data-parameters=\"feature=oembed&amp;wmode=opaque\"></div>\n<p>Slides: <a href=\"http://hdl.handle.net/2249.1/156346\" rel=\"nofollow noopener\">http://hdl.handle.net/2249.1/156346</a></p>\n<p>Jon is blogging about  Project ImPACT (Infrastructure for Privacy-Assured CompuTations) at <a href=\"http://cyberimpact.us/blog/\" rel=\"nofollow noopener\">http://cyberimpact.us/blog/</a></p>\n<p>Full disclosure that I\u2019m a developer for Dataverse, which is part of the architecture. I\u2019ll include an image below and from the Dataverse perspective, we are tracking this ongoing work at <a href=\"https://github.com/IQSS/dataverse/issues/5213\" rel=\"nofollow noopener\">https://github.com/IQSS/dataverse/issues/5213</a></p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://ask.cyberinfrastructure.org/uploads/default/original/1X/9557a78465cd2b2f50f4db692cbd701f4e444040.jpeg\" data-download-href=\"https://ask.cyberinfrastructure.org/uploads/default/9557a78465cd2b2f50f4db692cbd701f4e444040\" title=\"ImpactTRSAConcept-1024x724.jpg\"><img src=\"https://ask.cyberinfrastructure.org/uploads/default/original/1X/9557a78465cd2b2f50f4db692cbd701f4e444040.jpeg\" alt=\"ImpactTRSAConcept-1024x724\" width=\"690\" height=\"487\" data-small-upload=\"https://ask.cyberinfrastructure.org/uploads/default/optimized/1X/9557a78465cd2b2f50f4db692cbd701f4e444040_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">ImpactTRSAConcept-1024x724.jpg</span><span class=\"informations\">1024\u00d7724 130 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>"
    ],
    "580": [
        "<p>Hey everyone! I\u2019d like to develop some applications that use the Discourse API <a href=\"https://docs.discourse.org/\" rel=\"nofollow noopener\">https://docs.discourse.org/</a> to allow uesrs to generate / interact with content from places like the command line. It looks like a user with the admin panel can create a site wide api key, but then with a little extra tweaks can also create an endpoint so that a user can request one (see here <a href=\"https://meta.discourse.org/t/discourse-api-authentication/25941\" rel=\"nofollow noopener\">https://meta.discourse.org/t/discourse-api-authentication/25941</a>). Has anyone thought about this and would it be possible?</p>",
        "<p>Hey, so we did this! First we created the endpoint for a user to request an access token, this pull request here <a href=\"https://github.com/discourse/discourse/pull/6840\" rel=\"nofollow noopener\">https://github.com/discourse/discourse/pull/6840</a> and then (as an example) I added a discourse endpoint to the helpme client, here <a href=\"https://vsoch.github.io/helpme/helper-discourse\" rel=\"nofollow noopener\">https://vsoch.github.io/helpme/helper-discourse</a>. If you are a developer interested in seeing how it works (to develop your own applications to interact with discourse) then see the code base at <a href=\"https://www.github.com/vsoch/helpme\" rel=\"nofollow noopener\">https://www.github.com/vsoch/helpme</a>.</p>",
        "<p>This seems incredibly cool. Is there a live instance of helpme running that can interact with our Ask.CI Discourse installation?</p>",
        "<p>Helpme is a command line client, so you (as a user) install it with pip and then can use it. It doesn\u2019t need to have any server / instance running.</p>"
    ],
    "856": [
        "<p>I\u2019ve been seeing posts around like <a href=\"https://www.reddit.com/r/HPC/comments/bdf2eo/where_to_get_started_with_hpc_and_old_server_room/\" rel=\"nofollow noopener\">this</a> and I\u2019m genuinely interested in what it takes to create your own little cluster. A lot of these discussions start with the assumption that I have a crapton of old equipment lying around, or I have a general plan and need feedback on the details (e,g., what software to use? How to do networking?) but I\u2019m actually interested in a higher level description of how (someone like me) could deploy a cluster. What are the general steps? How do I make decisions? For example - how could I do this with raspberry PIs in my closet? How could I do it on a couple of instances in the Cloud over a weekend?</p>\n<p>I think this is also important to talk about so we can shed light on some of the real differences between what the cloud is calling HPC, and what is actually HPC.</p>",
        "<p>At this point in time I do everything with virtualization on my mac with something like vmware fusion. This gives the greatest flexibility without having to worry about unexpected costs in the cloud when one forgets to turn instances off and you don\u2019t have to carry around a pile of hardware (or stow it away in the closet).</p>\n<p>Just to get a general feel for things one can start by manually creating a basic \u201csystem\u201d with login node and 1-2 compute nodes. The login node can also double as shared storage for home directories, software installations, scheduler. Now you have a platform to experiment with linux, nfs (or BeeGFS, etc), slurm and the rest of the stack.</p>\n<p>By using service names for all the various layers such as login, xfer, storage, the basic architecture is abstracted away from the actual server or virtual the service is running on.</p>\n<p>From there go to a more sophisticated setup that starts with a pxe server to install the smallest possible images.   Puppet or another solution can then be used to apply specific configurations to each node to make them a login node, compute, file transfer.</p>\n<p>The catch?</p>\n<ol>\n<li>Having a laptop with enough storage and memory to run handful of virts.</li>\n<li>Or running on external storage, hopefully SSD on a faster interface.</li>\n<li>Setting up PXE under vmware and managing the virtual network can be a pain in the neck.</li>\n<li>Responsibility for learning the entire stack top to bottom (this is the interesting part however)</li>\n<li>Sometimes having \u201creal\u201d hardware can just be more fun.</li>\n</ol>",
        "<p>The terms \u201ccluster\u201d and \u201cHPC\u201d are about as useful as \u201ccloud\u201d at this point, they pretty much can describe anything someone wants to describe with them. If we strip away all the fluff,</p>\n<ul>\n<li>cluster: two or more hosts that cooperate on a computational problem.</li>\n<li>HPC cluster: two or more hosts which cooperate on a fine grained computational problem very efficiently</li>\n</ul>\n<p>Two hosts with an OS installed, copies of the same applications in the same locations and ssh that works between them are a cluster, so a basic setup for learning is simple to do using any two of anything: VM, pi, open-box special from BestBuy, cloud instance, those old laptops you can\u2019t bear to throw away,\u2026 can all be used to learn to do \u201ccluster\u201d computing. It\u2019s also entirely possible to learn everything you need to know about parallel computing on a single system now that everything under the sun has multiple cores and you can get a CUDA aware GPU in a laptop.</p>\n<p>From that basic starting point it\u2019s all turd polishing by adding as much extra stuff as desired:</p>\n<ul>\n<li>Scheduler (Slurm, MOAB/MAUI/torque, LSF, PBS, Condor\u2026)</li>\n<li>Shared $HOME (I like NFS but the next item can work for this)</li>\n<li>Shared Parallel Filesystem (BeeGFS, Lustre, GPFS,\u2026)</li>\n<li>Common software stack (lmod/modules, easybuild, spack, etc\u2026)</li>\n<li>Provisioning tool (Warewulf, XCat, a gazillion others)</li>\n<li>Configuration management (Saltstack, ansible, cfengine, puppet, chef,\u2026)</li>\n<li>Interconnect (Ethernet, Infiniband, proprietary foo)</li>\n<li>Grouchy HPC Sysadmin to tell users \u201cNO!\u201d <img src=\"https://ask.cyberinfrastructure.org/images/emoji/twitter/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\">\n</li>\n</ul>\n<p>There\u2019s nothing magical about clusters, although the marketing would have us believe otherwise. IMHO the most important thing is to keep a good grasp on the high level view of \u201cwhat do I want to accomplish with this?\u201d because the problem being solved should drive the cluster, not the other way around. If the goal is to become a cluster sysadmin, then hit every bullet point hard and try multiple tools for each. If the goal is to learn parallel programming, skip it all and just run MPI or whatever interests you on your daily driver system.</p>\n<p>I think the most important thing we can take home from that list is how critical it is that we keep my boss convinced that the last item there is the one that matters the most.</p>",
        "<p><a class=\"mention\" href=\"/u/griznog\">@griznog</a> had the main ingredients of a cluster. I want to point out that on the software side, one could look into the OpenHPC effort:</p>\n<p><a href=\"https://openhpc.community/\" class=\"onebox\" target=\"_blank\" rel=\"nofollow noopener\">https://openhpc.community/</a></p>\n<p>It does not provide the base OS, apparently. But it does provide a lot of the tools and commonly used software packages on HPC. This can significantly cut down the time to get the HPC started.</p>"
    ],
    "821": [
        "<p>I was reading this article <a href=\"https://www.hpe.com/us/en/insights/articles/8-ways-sci-fi-imagines-data-storage-1903.html\" rel=\"nofollow noopener\">https://www.hpe.com/us/en/insights/articles/8-ways-sci-fi-imagines-data-storage-1903.html</a> about different ways Sci-Fi has imagined data storage, and it occurred to me that this would be an interesting question to propose to HPC. Akin to how the print media mis-represents science, or how film glamorizes different aspects, what (do you think) are common misunderstandings or views that people have about HPC? For example, we can talk about a supercomputer generally:</p>\n<ol>\n<li>\n<p>Media: A supercomputer is an infinitely massive, and complex underground grid that goes on for miles with flashing lights and complicated command sequences</p>\n</li>\n<li>\n<p>Reality: A supercomputer is an air conditioned server room that has a bunch of smaller servers in racks that are configured together with software that is installed just like any other software.</p>\n</li>\n<li>\n<p>Misconception: I have to be a genius programmer to use a compute cluster.</p>\n</li>\n<li>\n<p>Reality: I can get started with relatively little knowledge, maybe just a small tutorial with instructions to login, look around, and submit a task.</p>\n</li>\n</ol>\n<p>A good way to think about it is, if there were a movie made about HPC, what would a person\u2019s expectations be, what would they see in the movie, and then what would they read on wikipedia (and be surprised).</p>",
        "<p>A very common misconception I encounter with newcomers is that if they have a large computational problem to solve, all they need is access to <em>HPC</em>. Simply copying over their code and running it on <em>HPC</em> will make it run <em>faster</em> and <em>better</em>. No additional effort needed. So in a sense, the misconception is that <em>HPC</em> is just a big computer, it is used just like one\u2019s laptop or workstation, with the only difference being that it is infinitely more powerful just because it is <em>big</em>. Some of them are stunned when they see that serial performance on a large many-core machine can be slower than their own computer. Some even feel cheated on due to these false expectations.</p>",
        "<p>A related issue is getting across the concept of nodes. Yes, you can ask for your program to run on four nodes, but if you run the same code that is on your laptop, three of your nodes are going to be sitting idle. I usually end up telling people that each node is like your laptop, and if you ask for four nodes you have four laptops to run your code on <img src=\"https://ask.cyberinfrastructure.org/images/emoji/twitter/slightly_smiling_face.png?v=9\" title=\":slightly_smiling_face:\" class=\"emoji\" alt=\":slightly_smiling_face:\"></p>"
    ],
    "443": [
        "<p>How to you support containers on at your research computing center?<br>\nDo you provide containers for users?<br>\nDo you help users create containers?</p>",
        "<p>Yes, we use singularity for containers, especially for tensorflow and for rstudio (server). There are several containers in the \u201ccommunity\u201d area that everybody can contribute to, and we direct users to dockerhub and <a href=\"https://singularity-hub.org/\">https://singularity-hub.org/</a> to find their own containers without needing to build their own. So far we have not really needed to help users build their own containers, but I think we will do some examples and webcasts explaining how to do it.</p>",
        "<p>Yes - we use Singularity.  We have instructions on how to create a Singularity container from Docker hub; we pull Singularity images from the Nvidia GPU Cloud for DL workflows like Tensorflow and Pytorch.  If you are curious we have documentation at:<br>\n<a href=\"https://docs.hpc.arizona.edu/display/UAHPC/Containers\" class=\"onebox\" target=\"_blank\" rel=\"nofollow noopener\">https://docs.hpc.arizona.edu/display/UAHPC/Containers</a><br>\nChris</p>",
        "<p>We also use Singularity containers.<br>\nAlthough we allow uses to \u201cbring their own containers,\u201d and have some limited documentation at<br>\n<a href=\"http://hpcc.umd.edu/help/software/singularity.html\">http://hpcc.umd.edu/help/software/singularity.html</a> on how to do so, I am unaware of anyone actually doing that.</p>\n<p>We have a handful of packages (mostly deep learning) which we only provide as containers.<br>\nWe provide environmental modules which load singularity if needed and set a variable with the path to the container,  and set the PATH to include some wrapper scripts.  The wrapper scripts will invoke the appropriate command in the container, and handle things like mounting lustre<br>\nfilesystem and nvidia drivers if a GPU node.  So e.g. for tensorflow all user needs to do is:<br>\nmodule load tensorflow<br>\ntensorflow myscript.py</p>",
        "<p>At Stanford on most of our clusters (Sherlock, Farmshare, SCG4) we support Singularity containers, and the \u201cbring your own container\u201d mentality. Our newly released <a href=\"https://www.sherlock.stanford.edu/docs/software/using/singularity/\" rel=\"nofollow noopener\">documentation is here</a>, and I also started a small effort called \u201c<a href=\"https://vsoch.github.io/containershare/\" rel=\"nofollow noopener\">containershare</a>\u201d that will allow a user to specify a shared space (my scratch in a share folder via a tool called forward (linked in the instructions for the containershare repository),</p>\n<p>I also single handedly manage Singularity Hub mentioned by <a class=\"mention\" href=\"/u/krisp\">@KrisP</a> , so hopefully this is also useful to provide containers on the fly, built version controlled from Github repos. I\u2019ll also point others on here to Singularity Registry, which is the open source Singularity Hub. It would let an institution deploy their own Singularity Hub, and instead of reliance on Github actually push containers directly to it via an authentication token. My hope is that institutions that want to provide containers for their users can have some server running with their own Singularity Registry, with containers then accessible also with the shub:// uri. Given that we then add <a href=\"http://schema.org\" rel=\"nofollow noopener\">schema.org</a> definitions for Containers to these pages, it would then be possible to have containers indexed with Google (akin to the Google Dataset search) and we could actually solve this discoverability problem. More info on that here --&gt; <a href=\"https://vsoch.github.io/2018/schemaorg/\" rel=\"nofollow noopener\">https://vsoch.github.io/2018/schemaorg/</a></p>\n<p>TLDR: let\u2019s work together on both the open source registry server and standards to describe the containers so we can not only support containers, but also their provenance and discoverability. It should be a community effort to work on these tools (and associated documentation) together.</p>",
        "<p>Thank you for this question - we have been working our way through the best approach for supporting containers as well.   At Northwestern we use Singularity on our HPC cluster.  We provide documentation (<a href=\"https://kb.northwestern.edu/page.php?id=85614\" rel=\"nofollow noopener\">https://kb.northwestern.edu/page.php?id=85614</a>) and teach workshops; while interest is high from our user community, it\u2019s still early-days for broad adaptation.</p>\n<p>Singularity-hub is a great resource (thank you <a class=\"mention\" href=\"/u/vsoch\">@vsoch</a>!) especially for the bioinformatics community.  Bioinformatics software has recently had an explosion of dependencies for installation - we\u2019ve seen as many as 900 for a single package - so we point  that community to containers whenever possible.</p>\n<p>We do build containers in some cases - for example if a software package doesn\u2019t run on our system.  We built a container from scratch for one very complex pipeline and it was a learning experience.  We realized that building containers from user\u2019s home-grown code wasn\u2019t a sustainable support model that we could offer.  By providing documentation and teaching workshops we hope our users can develop the skills to create their own containers.</p>",
        "<p>At UMD we also support Singularity containers on the HPC.<br>\nWe are a mix of \u201cbring-your-own\u201d and systems staff built containers; would like to go more toward the former but not all our users are comfortable building their own containers.  We build some containers for packages that do not install well on our cluster \u2014 we typically provide wrapper scripts and setup modules to make things somewhat seamless (some users might not even realize they are using containers); e.g<br>\nmodule load tensorflow<br>\ntensorflow my-script.py<br>\nwill load tensorflow module which loads Singularity, sets an env var with name to the container image file, and adds tensorflow script which launches tensorflow in the container image and passes arguments to it.</p>",
        "<p>For the Singularity admins, how much of a time commitment from the staff is required to install and maintain Singularity?</p>\n<p><a class=\"mention\" href=\"/u/krisp\">@KrisP</a><br>\n<a class=\"mention\" href=\"/u/chrisreidy\">@Chrisreidy</a><br>\n<a class=\"mention\" href=\"/u/payerle\">@payerle</a><br>\n<a class=\"mention\" href=\"/u/vsoch\">@vsoch</a><br>\n<a class=\"mention\" href=\"/u/jnugent\">@jnugent</a></p>",
        "<p>I don\u2019t directly maintain an installation, but I can comment on how the installations are maintained by my colleagues. First, if you have or are considering installing a version before 2.6.0, please don\u2019t. There were security issues with respect to (not all, but some) platforms and the runtime, and it\u2019s not worth the risk.</p>\n<h2>2.6.1</h2>\n<p>For 2.6.1 (no security issues) the installation is really easy - there are basic dependencies on the host like libtool, python, squashfs (for later versions of the 2 series) but the entire installation is via autotools and comes down to just a clone, configure, and install. You\u2019d need to grab a release or vault branch:</p>\n<pre><code class=\"lang-bash\">git clone -b 2.6-release https://www.github.com/sylabs/singularity\ncd singularity\n./autogen.sh\n./configure --prefix=/usr/local\nmake\nsudo make install\n</code></pre>\n<p>You can also build an RPM from the source code, and you would want to consider customizing the prefix or local prefix, or any of the defaults in the configuration file, as is typical. More details are <a href=\"https://www.sylabs.io/guides/2.6/admin-guide/admin_quickstart.html#installation\" rel=\"nofollow noopener\">here</a> and <a href=\"https://www.sylabs.io/guides/2.6/admin-guide/the_singularity_config_file.html\" rel=\"nofollow noopener\">here</a>. Note that those two links are from the \u201cAdmin Guide\u201d that you should look over to learn more about the software.</p>\n<h2>3.0+</h2>\n<p>After version 3.0.0 the code base switched to GoLang. It was likely to make the software more enterprise friendly, but (in the short term) hasn\u2019t added much in the way of features for HPC, and has made it much more challenging to install. GoLang is not as friendly to use on a shared HPC resource as something like Autotools. It comes down to needing to install GoLang, clone the repo to $GOPATH, and then use  ./mconfig to build and install. There are quite a few more dependency libraries, but these are all available via standard package managers. It\u2019s a bit rough figuring out how to install the GoLang dependencies, and I\u2019m hopeful that by using Go modules this will become easier. See <a href=\"https://github.com/sylabs/singularity/pull/3184\" rel=\"nofollow noopener\">https://github.com/sylabs/singularity/pull/3184</a></p>\n<p>For clusters that use spack, there is a (pre GoLang) spack install, and a pull request for 3.0+ <a href=\"https://github.com/spack/spack/pull/11094\" rel=\"nofollow noopener\">underway</a>. I\u2019m not entirely sure how this works if it\u2019s installed in user space (it typically doesn\u2019t) but my best guess is that it\u2019s installed with spack and sudo.</p>\n<p>In a nutshell, it\u2019s not absolutely terrible, but has gotten a little harder with recent versions. A few of our clusters use LMOD to manage loading Singularity, and since it\u2019s so popular some have it available by <a href=\"https://news.sherlock.stanford.edu/posts/sherlock-goes-container-native\" rel=\"nofollow noopener\">default</a>. If you do choose to install it, note that (historically) there have been more troubles with Centos hosts, and you will do your users a favor to export their SINGULARITY_CACHEDIR to somewhere like SCRATCH where there is a lot of space (HOME is likely to fill up and lock them out).</p>"
    ],
    "612": [
        "<p>I am looking into implementing either easybuild or spack at my institution. We specialize in genomic pipelines and bioinformatics and I am looking for opinions on which would be a better fit.</p>",
        "<p>I\u2019ve tried both of these but use neither on systems I manage. This response will be relative to build systems like these in general rather than at a specific tool.</p>\n<p>In my experience tools that add a meta layer to building software build the easy to build stuff that I don\u2019t need help with really well and the stuff I do need help with, they still require effort and in many cases only add an extra layer of abstraction and complexity to the building. The problem with a lot of software, especially bioinformatics software, is that it is poorly written and the install steps poorly documented, out of date or rapidly changing. An extra layer of abstraction that can also have the same problems doesn\u2019t really help with that.</p>\n<p>What has worked well for me is</p>\n<ol>\n<li>Manage software with <code>modules</code>\n</li>\n<li>Disk space is cheap, minimize inter-module dependencies by putting deps right into the module (same --prefix) for complex packages.</li>\n<li>Use what works best, e.g. a module which loads anaconda, then having users <code>source activate</code> from a collection of envs can be a timesaver for things that happen to install easily in a conda venv. Ditto for linuxbrew.</li>\n<li>Look for a dockerfile. Note I did NOT say use a container, they are rarely an improvement to any situation. But, the one thing docker has done is convince the container fanbois to write what are effectively executable and tested install instructions. It can be pretty easy to convert a dockerfile to a set of steps to use to just install something.</li>\n</ol>",
        "<p>I installed snp-pipeline very easily with Spack, and it has these dependencies:<br>\nBowtie2, SMALT, SAMtools, Picard, GATK, VarScan, tabix, bgzip, Bcftools, fastq-dump</p>\n<p>I never tried Easybuild.</p>"
    ],
    "676": [
        "<p>How do other HPC clusters offer access to researchers with jobs that regularly require many nodes for short periods of time?</p>\n<p>We offer:<br>\n1. a free tier model that provides up to 500,000 hours in a general access pool with lots of nodes, which can accommodate the bursty nature of these jobs well but only until they hit the 500,000 SU limit;<br>\n2. a paid condo model for researchers to purchase their own nodes for their exclusive access; the cost of purchasing many (20+) nodes is prohibitive, especially if they are not needed by the researcher most of the time.</p>\n<p>Neither of these models seem like a good fit for researchers who need access to many nodes on a regular basis but only for short periods of time.</p>\n<p>Does your institution have an option for researchers in this situation?</p>",
        "<p>We generally do not provide the \u201ccondo\u201d model but instead something (for lack of a better term) called the \u201ccoop\u201d model.  When researcher contributes funds, we purchase hardware to add to the cluster, but that hardware is NOT for the researcher\u2019s exclusive use.  It is added to a general pool, and the researcher gets<br>\nSUs, replenishing quarterly, proportional to the SUs added to the cluster by the hardware they contributed.<br>\nThese SUs can be used to run jobs on any hardware in the pool, although you are not guaranteed that jobs will start immediately.</p>\n<p>So if a researcher contributes 1 node to the cluster, he can do any of the following:</p>\n<ol>\n<li>run jobs on 1 node 100% of the time (for the entire quarter)</li>\n<li>run jobs on 2 nodes 50% of the time</li>\n<li>run jobs on 10 nodes 10% of the time, etc</li>\n</ol>\n<p>Depending on how heavily the cluster is being used, the amount of time jobs wait in queue can vary, but usually averages no more than a few hours for larger jobs.  Shorter jobs that don\u2019t need too many nodes can usually do better, sneaking in the backfill.</p>"
    ],
    "733": [
        "<p>Hey HPC nerds! I\u2019m putting together a little tutorial and introduction to job arrays (very simple) and the most important detail is having a list of compelling reasons we would want to use them in the first place, say, over something standard like submission with sbatch. Let\u2019s put our heads together and think! I\u2019m relatively new to using them so my list is likely limited.</p>\n<ul>\n<li>Running a randomized simulation many times, with output files numbered from some 1\u2026N. The output files can be numbered according to the array index. This means we use the array index as a variable to name our output files.</li>\n<li>Running an analysis over inputs, where each input is named according to the array index, and outputs follow suit. This could also be applied to directory names.</li>\n</ul>\n<p>What do others think?</p>",
        "<p>Any large data operation where the results are not dependent on each other and the input and outputs can be designated by the iterator.  Here\u2019s what we put together to demo the problem  and some approaches:</p>\n<p>In this case, the scheduler is SGE, but the approaches (and some of the problems addressed) are identical among schedulers:</p>\n<p><a href=\"https://goo.gl/cEWNJ6\" class=\"onebox\" target=\"_blank\" rel=\"nofollow noopener\">https://goo.gl/cEWNJ6</a><br>\nta,<br>\nHarry</p>",
        "<p>An example which reads multiple parameters from a separate file is handy, e.g. in the job script:</p>\n<pre><code>#!/bin/bash\n# An input file with a line for each array element\n# and parameters separated by spaces.\nPARAMS=/path/to/parameter/file.txt\nread -ra params &lt;&lt;&lt;\"$(sed ${SLURM_ARRAY_TASK_ID}'q;d' $PARAMS)\"\n</code></pre>\n<p>After which params will be an array with all the things from line ${SLURM_ARRAY_TASK_ID} in the $PARAMS file. By using an array it allows the number of parameters to change, so the next lines in the script might be</p>\n<p><code>command -t ${params[0]} -b ${params[1]} ${params[@]:2}</code></p>\n<p>to use the first two elements to set parameters and then pass everything else to the command.</p>\n<p>Once a parameter file is ready (by whatever means it is created) the job can then be submitted with</p>\n<p><code>sbatch --array=1-$(awk 'END{print NR}' /path/to/parameter/file.txt) job_array.sh</code></p>\n<p>edit: Although I use the line read from the file as parameters here, there\u2019s no reason why the entire command can\u2019t be in the parameter file, so the array can be used really to run any arbitrary set of commands as the tasks.</p>\n<p>Also when working on a cluster which limits the number of job array elements, using a step in the array spec and then having each array task run several commands for it\u2019s step can be useful. Say with</p>\n<p><code>--array=1-100:10</code></p>\n<p>Then each element can work on lines <code>${SLURM_ARRAY_TASK_ID}</code> through <code>${SLURM_ARRAY_TASK_ID} + 10</code> of the input file. Season to taste, of course.</p>"
    ],
    "89": [
        "<p>What are some examples of the first program people use as an example in HPC training classes?</p>\n<p><strong>CURATOR:</strong> Aaron Culich</p>",
        "<p>On the \u201cIntro to the SCC cluster environment\u201d we teach at BU we have 4 basic examples written in<br>\nC, python, R and matlab. During the tutorial we let attendees to chose one of them and use it as an example to submit jobs on our cluster.</p>",
        "<p>If the scope of \u201cHPC training\u201d is further limited to traditional parallel programming with C/C++/Fortran (be it multithreaded or multi-node), then there are a number of typical examples:</p>\n<ul>\n<li>Calculating pi with Monte Carlo</li>\n<li>Numerical integration of 1-D function</li>\n<li>Solving Laplace equation on a 2-D or 3-D grid</li>\n</ul>",
        "<p>There is  also <a href=\"https://projecteuler.net/\">https://projecteuler.net/</a></p>",
        "<p>I haven\u2019t given a training class, but I\u2019ve done a lot of \u201cintro\u201d exercises on this platform called CodeStepByStep --&gt; <a href=\"http://codestepbystep.com\" rel=\"nofollow noopener\">codestepbystep.com</a> and I think it would be <em>so</em> useful to have a similar tool to as people questions (and test answers) for simple linux, slurm (job manager) and then version control commands. It\u2019s pretty fun to do, easy in a browser, and I\u2019d love to work on this with someone if there is interest!</p>",
        "<p>We have a github repo with example jobs using different programs that users can copy or reference for different situations: <a href=\"https://github.com/nuitrcs/examplejobs\" rel=\"nofollow noopener\">https://github.com/nuitrcs/examplejobs</a></p>\n<p>In our intro workshop, we just use a basic helloworld script: <a href=\"https://github.com/nuitrcs/intro-quest-workshop\" rel=\"nofollow noopener\">https://github.com/nuitrcs/intro-quest-workshop</a></p>"
    ],
    "129": [
        "<p><strong>CURATOR:</strong> John Goodhue</p>\n<p><strong>COMMENT-JG</strong> I suggest putting this in the discussion zone for now.  This is an important topic area, but needs narrower questions that focus on specific tools or techniques.  The remarks from@pdurbin below point to some useful specifics.</p>\n<p><strong>Editor remarks - Pdurbin:</strong> The question \u201cHow do I get my software outputs cited as scholarly work?\u201d sounds a bit like \u201cHow do I increase my h-index?\u201d It\u2019s one thing to make your software citable, but it\u2019s quite another to actually get cited. The abstract of \u201cSoftware citation principles\u201d at <a href=\"https://peerj.com/articles/cs-86/\" rel=\"nofollow noopener\">https://peerj.com/articles/cs-86/</a> begins with \u201cSoftware is a critical part of modern research and yet there is little support across the scholarly ecosystem for its acknowledgement and citation.\u201d So it\u2019s an uphill battle. That paper recommends making sure you software has a \u201cunique, persistent, and machine-actionable identifiers such as a DOI, ARK, or PURL\u201d and if your software is on GitHub, the most straightforward way to give it a DOI is to use Zenodo as explained at <a href=\"https://guides.github.com/activities/citable-code/\" rel=\"nofollow noopener\">https://guides.github.com/activities/citable-code/</a></p>",
        "<p>I really like the Journal of Open Source Software, which will guide you to creating a zenodo doi too. The process comes down to submitting a Github repository with a paper (that is well defined how to create it) and then going for a reliable checklist to do the peer review. See: <a href=\"https://joss.theoj.org/\" rel=\"nofollow noopener\">https://joss.theoj.org/</a></p>\n<p>Specifically, it will give you:</p>\n<ol>\n<li>version control by default of your code, since you submit from Github</li>\n<li>version control for the paper as well (in markdown and bib file from the same repository)</li>\n<li>a really great peer review in the way of a discussion over Github issues. I\u2019ve had, by far, the best peer reviewed work on JOSS over any (traditional) journal</li>\n<li>very quick review, depending on the reviewers of course. I\u2019ve had an entire thing done in a week or two, and this is more common than not.</li>\n<li>transparency in every way, as issues and discussion are public</li>\n<li>a focus on the software, documentation, notes for contribution, and a brief background without a lot of the fluff that a traditional paper would require (meaning, you can demonstrate it\u2019s needed and works without needing an entire study of some domain with p &lt; 0.05)</li>\n</ol>\n<p>And at the end of all of that, you get a peer reviewed software base, with the same \u201cthing to cite\u201d as from a journal.</p>"
    ],
    "84": [
        "<p>Does anybody have a simple parallel program for use in teaching Nuclear Engineering students who are just learning to get started with HPC resources?</p>\n<p><strong>CURATOR:</strong> Grace Wilson Caudill</p>",
        "<p><a class=\"mention\" href=\"/u/gwcunhcie\">@gwcunhcie</a> It would help to know what kind of use case are you trying to build to, for example Using Arrays-jobs, Using MPI commands to run an existing program, writing their own MPI programs?  And what (if any) background is you typical student starting with?</p>",
        "<p>A lot of these \u201cI need an intro or learning interface\u201d make me think that \u2026 we need just that. <img src=\"https://ask.cyberinfrastructure.org/images/emoji/twitter/slight_smile.png?v=6\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"> Akin to Canvas, or CodeStepByStep, it would be really useful to have an online collaborative platform (I did some searching and traditional ones are called LMS for Learning Management Service) to casually ask simple questions, validate answers, and make it fun and easy so users want to use it. I personally think a fullblown classroom / course application is overkill, I give CourseStepBytep as an example because it\u2019s really easy to log in, choose your language of choice, and just complete little simple problems with no aim other than the satisfaction of green validation passing and then a record that you\u2019ve done a lot. What do others thing? I think ideally it would be setup akin to discourse, or codestepbystep, and be very easy to contribute content or log in as a user to generate questions for.</p>"
    ],
    "91": [
        "<p>What job managers are available, how do I easily move between them, and how do they map to tools in the cloud?</p>\n<p>I am currently using PBS to run my analysis and want to migrate to the cloud to collaborate for a multi-institution project. Is there any recommendation on the type of cloud service? And what tools I will need to learn to help with this migration.</p>\n<p><strong>CURATOR:</strong> Raminder Singh</p>",
        "<p>When considering whether and how to migrate your workload from an HPC cluster environment to the cloud you may first wish to check to see if your campus research computing group provides consulting to help with that.</p>\n<p>There are also resources beyond the campus at the national scale that may help such as the XSEDE <a href=\"https://www.xsede.org/for-users/ecss\">Extended Collaborative Support Service (ECSS)</a> and the <a href=\"https://sciencegateways.org/\">Software Community Gateways Institute (SGCI)</a>. Those services are both available to help you determine what\u2019s needed for a multi-institution project, especially if you have a workflow suitable as a Science Gateway that provides a web-frontend to HPC cluster resources (so that you don\u2019t have to change your workflow, except to maybe switch to a different job scheduler such as SLURM available on the newer XSEDE clusters), or whether your workflow is better suited to a cloud environment that may not require a batch scheduler at all, but may require you to change your workflows and related software.</p>",
        "<p>I\u2019d recommend looking into intra/extra institutional support First (such as in <a class=\"mention\" href=\"/u/aculich\">@aculich</a>\u2019s great answer for an example).</p>\n<p>That said, There is nothing specific to the Cloud that will keep you from rolling your own setup using what ever you want.</p>\n<p>Additionally, pretty much all the cloud vendors also support, provide or encourage tools for easy setup and config with common schedulers. As well as alternatives based on server-less and/or container models, and pre-built \u2018marketplace\u2019 options</p>\n<p><em>The tools according to the big three</em>:<br>\ncnfCluster  <a href=\"https://cfncluster.readthedocs.io/en/latest/\" rel=\"nofollow noopener\">https://cfncluster.readthedocs.io/en/latest/</a><br>\nAWS supported</p>\n<p>elastiCompute <a href=\"http://gc3-uzh-ch.github.io/elasticluster/\" rel=\"nofollow noopener\">http://gc3-uzh-ch.github.io/elasticluster/</a><br>\nGCE recommended (can also be used with AWS &amp; private OpenStacks)</p>\n<p><aside class=\"onebox whitelistedgeneric\">\n  <header class=\"source\">\n      <img src=\"https://github.githubassets.com/favicon.ico\" class=\"site-icon\" width=\"32\" height=\"32\">\n      <a href=\"https://github.com/Azure/azure-quickstart-templates/tree/master/create-hpc-cluster-linux-cn\" target=\"_blank\" rel=\"nofollow noopener\">GitHub</a>\n  </header>\n  <article class=\"onebox-body\">\n    <img src=\"https://ask.cyberinfrastructure.org/uploads/default/original/1X/5fa07a575b33778e174c80c2e1285c1c8d4c3223.png\" class=\"thumbnail onebox-avatar\" width=\"380\" height=\"380\">\n\n<h3><a href=\"https://github.com/Azure/azure-quickstart-templates/tree/master/create-hpc-cluster-linux-cn\" target=\"_blank\" rel=\"nofollow noopener\">Azure/azure-quickstart-templates</a></h3>\n\n<p>Azure Quickstart Templates. Contribute to Azure/azure-quickstart-templates development by creating an account on GitHub.</p>\n\n\n  </article>\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n  <div style=\"clear: both\"></div>\n</aside>\n<br>\nAzure</p>\n<p>all of which officially support SGE, Slurm, and Torque (PBSpro is an active request in both elastiCompute and cnfCluster)</p>\n<p>note: I\u2019ve personally only dealt with AWS &amp; GCE, mostly with SGE so YMMV.</p>",
        "<p>A couple of quick comments and then an answer</p>\n<p><strong>Comments</strong></p>\n<ol>\n<li>\n<p>It is hard to tell what question is being asked from this post. It might be worth rewording to make it easier to get a focussed answer. The most useful answer may depend on what sort of workflow and what application/problem you are trying to solve. If its a specific question about spinning up a virtual cluster with a specific scheduler and software stack then the question should clarify that.</p>\n</li>\n<li>\n<p><a class=\"mention\" href=\"/u/raminder\">@raminder</a> I think the post title might be better reading</p>\n<blockquote>\n<p>What HPC job managers are available on cloud platforms?</p>\n</blockquote>\n<p>as it it currently written it is not a question.</p>\n</li>\n<li>\n<p>The explanatory sentence</p>\n<aside class=\"quote no-group\" data-post=\"1\" data-topic=\"91\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt width=\"20\" height=\"20\" src=\"https://ask.cyberinfrastructure.org/letter_avatar_proxy/v2/letter/r/58956e/40.png\" class=\"avatar\"> raminder:</div>\n<blockquote>\n<p>What job managers are available, how do I easily move between them, and how do     they map to tools in the cloud?</p>\n</blockquote>\n</aside>\n<p>does not make sense to me.  Are you asking <code>what job managers are available</code> anywhere and how you move between SLURM, PBS, SGE, LSF etc\u2026 or are you asking about using job managers in the cloud and moving between cloud providers. The text isn\u2019t very clear.</p>\n</li>\n<li>\n<p>Overall it might be useful to clarify whether this is a narrow specific question such is \u201cIs there a recipe I can use to create a PBS cluster on AWS?\u201d or whether the question is broader like \u201cWhat cloud platforms can I use for a PBS workflow that I currently execute on a bare metal cluster?\u201d.</p>\n</li>\n</ol>\n<p><strong>Now an answer</strong></p>\n<p>Assuming you are asking about using cloud resources with traditional HPC style workload managers then the answer is that nearly all cloud platforms in their most basic form provide networked virtual machines. As such you can configure pretty much any scheduler software that you can use on networked physical machines. As noted above</p>\n<aside class=\"quote no-group\" data-post=\"3\" data-topic=\"91\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt width=\"20\" height=\"20\" src=\"https://ask.cyberinfrastructure.org/letter_avatar_proxy/v2/letter/j/96bed5/40.png\" class=\"avatar\"> jpessin1:</div>\n<blockquote>\n<p><em>The tools according to the big three</em> :<br>\ncnfCluster <a href=\"https://cfncluster.readthedocs.io/en/latest/\" rel=\"nofollow noopener\">https://cfncluster.readthedocs.io/en/latest/</a><br>\nAWS supported</p>\n<p>elastiCompute <a href=\"http://gc3-uzh-ch.github.io/elasticluster/\" rel=\"nofollow noopener\">http://gc3-uzh-ch.github.io/elasticluster/</a><br>\nGCE recommended (can also be used with AWS &amp; private OpenStacks)</p>\n</blockquote>\n</aside>\n<p>each of AWS, GCE and Azure provide some tools to streamline the process of launching multiple virtual machines in a manner that they can form a cluster.</p>\n<p>There are also a number of independent tools that can perform similar tasks. Some of these include</p>\n<ul>\n<li>\n<p><a href=\"http://star.mit.edu/cluster/\" rel=\"nofollow noopener\">StarCluster</a>, an early effort but still widely used.</p>\n</li>\n<li>\n<p><a href=\"https://www.terraform.io\" rel=\"nofollow noopener\">Terraform</a>, a more general tool</p>\n</li>\n</ul>\n<p>You can also script the steps of creating a cluster with some work yourself.</p>\n<p>If you are more looking for a service then there are several activities trying to address this. For example</p>\n<ul>\n<li>\n<a href=\"https://www.rescale.com\" rel=\"nofollow noopener\">Rescale</a> provide a commercial service that utilizes cloud resources to streamline running many typical research computing application (e.g. <a href=\"http://gaussian.com\" rel=\"nofollow noopener\">Gaussian</a>, <a href=\"http://www.ks.uiuc.edu/Research/namd/\" rel=\"nofollow noopener\">NAMD</a> etc\u2026) workflows that are normally executed through job managers like PBS.</li>\n</ul>",
        "<p>Another couple of options to mention are alces flight (AWS &amp; Azure) and CloudyCluster (AWS and soon GCP, autoscaling with the CCQ meta-scheduler).</p>"
    ],
    "441": [
        "<p>Does anyone have any ballpark figures for the power bill for running hpc nodes ?</p>",
        "<p>Nodes your looking at peak power draw of about 1KW/4 2 socket nodes.  This assumes no GPU\u2019s or large number of memory modules etc.   This is peak actual measured, likely your actuals will be much lower and also account for how well utilized your nodes are.  Also if you use 200W CPU\u2019s your peak might be higher.</p>\n<p>Also this is just the node.  If your looking at TCO you need to include your data center power for cooling.  Ask your facilities people for the PUE of the room: <a href=\"https://en.wikipedia.org/wiki/Power_usage_effectiveness\" rel=\"nofollow noopener\">https://en.wikipedia.org/wiki/Power_usage_effectiveness</a></p>\n<p>Most standard data centers without air containment, or liquid cooling, your probably looking at a PUE of 2.  So that would double the total power to run a node.</p>\n<p>For actual cost also ask your facilities.  Often data centers are costed at comercial rates as you own your own distribution which lowers the cost.  Eg were $0.08/kw-hr  while at my house were $0.14/kw-hr.</p>\n<p>Talk with your vendor, their numbers are always very conservative, so they will report much higher power draw peak than you likely will see in practice even win HPC at 100% CPU load.</p>"
    ],
    "427": [
        "<p>I encountered a problem with extremely large neural network that was created in KERAS, using Tensorflow backend. The memory footprint in one of the layer is already bigger than the size of current GPU memory (it has just over 4 billion parameters \u2013 which, using float32, translates to a matrix 16 GB alone). Is there a neural network implementation that can get around this problem? For example, will TensorFlow accommodate such a case? There are papers/discussions out there on handling matrix multiply that is greater than GPU\u2019s memory size (basically that is done by tiling the matrix and stream the data). Also there is a paper on virtual Deep NN that claims to be transparent to end-user as far as the use of CPU &amp; GPU memory:</p>\n<p>\u201cTraining Deeper Models by GPU Memory Optimization on TensorFlow\u201d<br>\n<a href=\"http://learningsys.org/nips17/assets/papers/paper_18.pdf\" class=\"onebox\" target=\"_blank\" rel=\"nofollow noopener\">http://learningsys.org/nips17/assets/papers/paper_18.pdf</a></p>\n<p>\" vDNN: Virtualized Deep Neural Networks for Scalable, Memory-Efficient Neural Network Design\"<br>\n<aside class=\"onebox pdf\">\n  <header class=\"source\">\n      <a href=\"https://arxiv.org/pdf/1602.08124.pdf\" target=\"_blank\" rel=\"nofollow noopener\">arxiv.org</a>\n  </header>\n  <article class=\"onebox-body\">\n    <a href=\"https://arxiv.org/pdf/1602.08124.pdf\" target=\"_blank\" rel=\"nofollow noopener\"><span class=\"pdf-onebox-logo\"></span></a>\n<h3><a href=\"https://arxiv.org/pdf/1602.08124.pdf\" target=\"_blank\" rel=\"nofollow noopener\">1602.08124.pdf</a></h3>\n\n<p class=\"filesize\">2.52 MB</p>\n\n  </article>\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n  <div style=\"clear: both\"></div>\n</aside>\n</p>\n<p>\u201cHow to Train a Very Large and Deep Model on One GPU?\u201d<br>\n<aside class=\"onebox whitelistedgeneric\">\n  <header class=\"source\">\n      <img src=\"https://ask.cyberinfrastructure.org/uploads/default/original/1X/8c7390150cd0078e967a18c7663581b9b334d239.png\" class=\"site-icon\" width=\"32\" height=\"32\">\n      <a href=\"https://medium.com/syncedreview/how-to-train-a-very-large-and-deep-model-on-one-gpu-7b7edfe2d072\" target=\"_blank\" title=\"03:16PM - 29 April 2017\" rel=\"nofollow noopener\">Medium \u2013 29 Apr 17</a>\n  </header>\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690/361;\"><img src=\"https://miro.medium.com/max/1200/0*3yQqm76EvWtk75Fh.\" class=\"thumbnail\"></div>\n\n<h3><a href=\"https://medium.com/syncedreview/how-to-train-a-very-large-and-deep-model-on-one-gpu-7b7edfe2d072\" target=\"_blank\" rel=\"nofollow noopener\">How to Train a Very Large and Deep Model on One GPU?</a></h3>\n\n<p>Problem: GPU memory limitation</p>\n\n  <p><span class=\"label1\">Reading time: 7 min read</span>\n    </p>\n\n  </article>\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n  <div style=\"clear: both\"></div>\n</aside>\n</p>\n<p>but my question is simply: is this doable using current neural network implementation? Tensorflow claims to support parallel computation, multiple GPU, etc. Will Tensorflow accommodate cases like that one above without choking?</p>",
        "<p>Thanks for sharing the paper and reading. It\u2019s really interesting. While doing research on this, I found <a href=\"https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9\">https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9</a> which may be useful for you.</p>"
    ],
    "394": [
        "<p>There is an increasing need to analyze sensitive data using shared cluster resources. Sensitive data is defined here as data that may fall under HIPAA, ITAR, FISMA, FERPA, etc guidelines. What approaches are in use besides just setting up isolated environments?</p>",
        "<p>Setting a dedicate onclave for protected data computation seems to be the prevailing way to provide the required protection measures. This includes: separate network, encryption in-network and at rest, and tight control of open ports, 2FA / MFA\u2026</p>\n<p>It may be helpful to list current efforts that people have undertaken in this area. I list what I am aware of, just in the spirit to be helpful. I have no affiliation or connection with these people. Here is the most recent workshop on HPC Security and Compliance at PEARC\u201918:</p>\n<p><a href=\"https://www.rc.ufl.edu/research/events/workshop-pearc18/\" class=\"onebox\" target=\"_blank\">https://www.rc.ufl.edu/research/events/workshop-pearc18/</a></p>\n<p>It is full of useful information. CTSC also has several presentations:</p>\n<ul>\n<li>Aug 27, \u201cNIST 800-171 Compliance Program at University of Connecticut\u201d</li>\n</ul>",
        "<p>There are a few schools of thought around this other than the enclaves.</p>\n<ul>\n<li>Single use nodes<br>\n** Nodes only run one job at a time, and they reload after each job and use an encryption key generated at boot.  Be sure to also use tools like pbs_pam and others so non-admins cannot connect to the node.</li>\n</ul>\n<p>For shared storage be sure to monitor that users do not modify permissions so they are readable by anyone other than the user.  Set umask to 077 etc.</p>\n<p>Aggressive off-boarding is also highly recommended.</p>\n<p>Depending on your circle of trust, that might be enough, or you could get away with less.  It depends on your risk appetite and DUA\u2019s.</p>"
    ],
    "368": [
        "<p>I\u2019m wondering how various institutions are handling questions of deploying notebook-type technologies at scale, especially in terms of their economics. In particular, we would like to deploy JupyterHub as a campus service. Right now, it\u2019s mainly been used as part of a pilot in two courses, with a total of about 60 students. The pilot has been rather successful, and we would like to make it available to the campus at large. We are also in the process of evaluating RStudio Server Pro, which is similar, and might be considering a deployment for broader campus use soon.</p>\n<p>My problem is that these technologies don\u2019t seem to scale well. My research and conversations with JH developers indicate that each session requires about 1GB of memory as a reasonable floor, and it\u2019s common to cap at about 2GB. The issue is that for 90% or more of the semester, we might see 0-4 concurrent sessions, and such traffic can be handled easily by a comparatively small server/VM. But - students being students - they will regularly want until the night before to work on homework or projects, and it\u2019s not uncommon to see 40-60 concurrent sessions. We are in a classic problem of needing a high amount of resources for a really small sliver of time.</p>\n<p>We don\u2019t have a system on-campus that can be dedicated to this, as it would require 128GB or so of memory to be allocated. (Cores and disk space are more trivial, but are not completely excluded; it\u2019s just that RAM is the primary limiting factor.) For example, Digital Ocean\u2019s \u201cdroplet\u201d VM that offers 128GB of memory comes in around $640/mo., whereas most of our typical concurrent needs could be met with, say, a VM with 16GB of RAM for $80/mo. There seems to be no effective way to \u201cburst up\u201d for those specific times that require additional resources.</p>\n<p>I thought, perhaps, that I could accomplish this with Kubernetes through Google Cloud, but that doesn\u2019t seem to be an option, really. I can provision a few smaller nodes and allow Kubernetes to \u201cload balance\u201d the requests, but it looks like I pay for the <em>availability</em> of those rather than their actual usage. And in this case, it comes out to somewhere around $700/mo. to offer a Kubernetes cluster with three smaller VMs.</p>\n<p>Ultimately, I\u2019d like a solution where I can burst up when needed, while paying more or less some base fixed cost plus whatever is needed to support the additional intermittent load. Because if we are facing these struggles trying to provide a service for 60 students, I can\u2019t imagine trying to offer something that upwards of 200-300 students might need each semester.</p>\n<p>I would welcome any thoughts or guidance.</p>\n<p>Warmest regards,<br>\nJason Simms</p>",
        "<p>We are currently using Open OnDemand to launch Jupyter notebooks in general, rather than JupyterHub. Though we have both JupyterHub and Open Ondemand, currently JupyterHub is just a VM and will probably not be able to sustain a lot of students. So, Open Ondemand is a better choice.</p>\n<p>We are looking into using batch spawner to launch Jupyter notebooks on the cluster as well.  One thing that Jupyterhub does better is incorporating nbgrader, which makes it easy to assign, collect and grade assignments. Though nbgrader could also be set up on a cluster, I have not yet made it so. (e.g. One class is fine, but what about several concurrent classes?)</p>\n<p>Another possibility is XSEDE JetStream instead of GCP for Kubernetes, which would solve the cost problem. I\u2019d love to learn about how to launch Kubernetes on JetStream!</p>",
        "<p>hmmm \u2026 that kind of sounds like it might be config issue \u2026</p>\n<p>GCP (including the Kubernetes engine) bills primarily on occupancy,  is it possible parts of your system are staying up?</p>\n<p>Though As KrisP mentioned jetstream is a good resource, with help and experience attached.</p>"
    ],
    "197": [
        "<p>Looks like training and education in the HPC/research computing realm needs its own place. This will be an area where one asks questions about approaches in training and education.</p>",
        "<p>I\u2019m sure I\u2019m missing a lot, but to get things started . . .</p>\n<p>and existing good list <a href=\"http://hpcuniversity.org/educators/programs/\" rel=\"nofollow noopener\">http://hpcuniversity.org/educators/programs/</a><br>\nand</p>\n<p>For users:<br>\n<a href=\"https://www.citutor.org/login.php\" class=\"onebox\" target=\"_blank\" rel=\"nofollow noopener\">https://www.citutor.org/login.php</a><br>\n<a href=\"https://hpc.llnl.gov/training\" class=\"onebox\" target=\"_blank\" rel=\"nofollow noopener\">https://hpc.llnl.gov/training</a><br>\n<a href=\"https://portal.xsede.org/online-training\" class=\"onebox\" target=\"_blank\" rel=\"nofollow noopener\">https://portal.xsede.org/online-training</a><br>\n<a href=\"http://hpcuniversity.org/resources/search/\" class=\"onebox\" target=\"_blank\" rel=\"nofollow noopener\">http://hpcuniversity.org/resources/search/</a><br>\n<a href=\"http://hpcuniversity.org/educators/programs/\" class=\"onebox\" target=\"_blank\" rel=\"nofollow noopener\">http://hpcuniversity.org/educators/programs/</a><br>\n<a href=\"https://nsdl.oercommons.org\" class=\"onebox\" target=\"_blank\" rel=\"nofollow noopener\">https://nsdl.oercommons.org</a></p>\n<p>For instructors and facilitators and more:<br>\n<a href=\"http://www.shodor.org/\" class=\"onebox\" target=\"_blank\" rel=\"nofollow noopener\">http://www.shodor.org/</a><br>\n<aside class=\"onebox whitelistedgeneric\">\n  <header class=\"source\">\n      <img src=\"https://ask.cyberinfrastructure.org/uploads/default/original/1X/03d868f8f6cfb2fe3153b6d97e5e990332d2741a.png\" class=\"site-icon\" width=\"32\" height=\"32\">\n      <a href=\"https://carpentries.org/\" target=\"_blank\" rel=\"nofollow noopener\">The Carpentries</a>\n  </header>\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690/362;\"><img src=\"https://ask.cyberinfrastructure.org/uploads/default/original/1X/e71b503931d9a50906c1d8e8c3b5d9d2f19b1701.png\" class=\"thumbnail\"></div>\n\n<h3><a href=\"https://carpentries.org/\" target=\"_blank\" rel=\"nofollow noopener\">The Carpentries</a></h3>\n\n<p>The Carpentries is a fiscally sponsored project of Community Initiatives, a registered 501(c)3 non-profit organisation based in California, USA. We are a global community teaching foundational computational and data science skills to researchers in...</p>\n\n\n  </article>\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n  <div style=\"clear: both\"></div>\n</aside>\n</p>"
    ],
    "245": [
        "<p>I\u2019m trying to conduct anomaly detection of certain features, based on 4,000 JPEG images of human cells.  What image-processing tools will be most efficient on my large data-sets?</p>",
        "<p>I\u2019m also investigating this for use in our cluster and here is what I found (keep in mind that I\u2019m not sure how it works on XSEDE):</p>\n<ul>\n<li>imageMagick is a general purpose program that can be used for a lot of image manipulation tasks. I wrote an example batch slurm script here for cropping to illustrate submitting an array job: <a href=\"https://github.com/rutgers-oarc/training/tree/master/slurm_examples/image_cropping\">https://github.com/rutgers-oarc/training/tree/master/slurm_examples/image_cropping</a>\n</li>\n<li>scikit-image is a python library that lets you do some image manipulation. Again, you would write a script that takes a single image as an input (or a small list of images) and then farm it out as an array job. There may be some throttling on XSEDE insuring that you don\u2019t flood the system with your jobs, so will only let you submit some number of tasks at once. Here are some options <a href=\"https://slurm.schedmd.com/job_array.html\">https://slurm.schedmd.com/job_array.html</a> and specifically <code>A maximum number of simultaneously running tasks from the job array may be specified using a \"%\" separator. For example \"--array=0-15%4\" will limit the number of simultaneously running tasks from this job array to 4.</code>\n</li>\n<li>openCV is another set of tools, very general and powerful, and installable as a python package</li>\n</ul>\n<p>Note: to install a python package only for the user in a shared cluster you would do <code>pip install --user mypythonpackage</code>.</p>"
    ],
    "154": [
        "<p>I\u2019m interested in publishing research data but I\u2019m not sure where to begin. What is some introductory material (websites, papers, etc.) I can read to learn more about this topic?</p>\n<p>I noticed that there is a related question at <a href=\"https://academia.stackexchange.com/questions/59664/how-to-share-a-scientific-dataset-with-the-research-community\" rel=\"nofollow noopener\">https://academia.stackexchange.com/questions/59664/how-to-share-a-scientific-dataset-with-the-research-community</a></p>\n<p><strong>CURATOR:</strong> Phil Durbin</p>",
        "<p>Moving to Discussion Zone - this is fairly broad topic but new questions can be generated for specific types of data.</p>"
    ],
    "99": [
        "<p>Can I run HPC software on cloud resources?</p>\n<p><strong>CURATOR:</strong> Raminder Singh</p>",
        "<p>Cost aside, the answer is mostly \u201cyes, to a point,\u201d but it also depends on which cloud, setup and job type.</p>\n<p>They will generally all work just fine if the jobs are independent or loosely coupled. Points of caution include: heavy network or disk I/O - most instances will have limits that lead to bottlenecks at lower usages than most on-premise HPC\u2019s.</p>"
    ],
    "273": [
        "<p>We occasionally get \u2018emergency\u2019 requests from users, asking for additional resources, or more often bumps in priority. How can we help, without being unfair to other users?</p>",
        "<p>There are two parts to this question: 1) how to manipulate the system to allow a user or group to run their jobs sooner and 2) how to do so <em>fairly</em> or <em>equitably</em>.</p>\n<p><strong>Manipulating things to favor a group or user</strong>:<br>\nDepending on your setup you can make (temporary) adjustments to the users\u2019 queuing priorities, accessible queues, possibly within projects if you\u2019re using them:</p>\n<p><a href=\"http://gridscheduler.sourceforge.net/htmlman/htmlman5/sge_priority.html\" class=\"onebox\" target=\"_blank\">http://gridscheduler.sourceforge.net/htmlman/htmlman5/sge_priority.html</a><br>\nwith <a href=\"http://gridscheduler.sourceforge.net/htmlman/htmlman1/qalter.html\">http://gridscheduler.sourceforge.net/htmlman/htmlman1/qalter.html</a> if the target job was already submitted.</p>\n<p>Or if you are using projects:<br>\n<a href=\"http://gridscheduler.sourceforge.net/htmlman/htmlman5/project.html\" class=\"onebox\" target=\"_blank\">http://gridscheduler.sourceforge.net/htmlman/htmlman5/project.html</a></p>\n<p>If you want to increase the concurrent job limits look at maxujobs \u2013 there are also many other options such as under weighting:</p>\n<p><a href=\"http://gridscheduler.sourceforge.net/htmlman/htmlman5/sched_conf.html\" class=\"onebox\" target=\"_blank\">http://gridscheduler.sourceforge.net/htmlman/htmlman5/sched_conf.html</a></p>\n<p>If XForwarding is enabled on your system, qmon wraps most of these in a GUI tool:</p>\n<p><a href=\"http://gridscheduler.sourceforge.net/htmlman/htmlman1/qmon.html\" class=\"onebox\" target=\"_blank\">gridscheduler.sourceforge.net/htmlman/htmlman1/qmon.html</a></p>\n<p><strong>Doing so \u201cequitably\u201d</strong>\u2026<br>\n\u2026is problematic.<br>\n<em>(Just my two cents, here)</em></p>\n<p>In most cases, within a given HPC system, either there is enough spare capacity to handle the jobs, in which case it is a non-issue OR there isn\u2019t, in which case any added/new resources the emergency request receives are essentially removed from other users.</p>\n<p>While this may be appropriate in a given case, if the normal settings are equitable, then I would doubt most would consider it fair to add a bias in someone\u2019s favor.</p>\n<p>The only exceptions are edge cases where there is both enough room for all, AND this job needs administrator level help to run (though this might also be a sign of a configuration/usecase mismatch).</p>"
    ]
}